{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "import os;\n",
    "\n",
    "os.environ[\"ACCELERATE_ENABLE_RICH\"] = \"0\"\n",
    "\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = os.path.basename(globals()['__vsc_ipynb_file__']) \n",
    "\n",
    "# os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "\n",
    "import sys\n",
    "from functools import partial\n",
    "import json\n",
    "from typing import List, Tuple, Union, Optional, Callable, Dict\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import circuitsvis as cv\n",
    "import webbrowser\n",
    "from IPython.display import display\n",
    "from transformer_lens import utils, ActivationCache, HookedTransformer, HookedTransformerConfig\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens.components import LayerNorm\n",
    "from eindex import eindex\n",
    "\n",
    "\n",
    "from sklearn.cluster import DBSCAN, HDBSCAN, OPTICS\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# t.set_grad_enabled(False)\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = r\"chapter1_transformers\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = exercises_dir / \"monthly_algorithmic_problems\" / \"september23_sum\"\n",
    "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "from monthly_algorithmic_problems.september23_sum.model import create_model\n",
    "from monthly_algorithmic_problems.september23_sum.training import train, TrainArgs\n",
    "from monthly_algorithmic_problems.september23_sum.dataset import SumDataset,Pairs\n",
    "from plotly_utils import hist, bar, imshow\n",
    "\n",
    "# Running this on a macbook air and mps is flaky\n",
    "device = t.device(\"cpu\") #t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import faiss\n",
    "\n",
    "\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "-- Note: I rewrote the Dataset class so that it balances no carry, plain carry and cascading carry  classes evenly. this puts a larger emphasis difficult cases relative to the original way this dataset was written. With random additions, cascading carry is an edge case that is quite infrequent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SumDataset(size=3000, num_digits=4).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "-- Note: I removed weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = section_dir / \"sum_model_normal.pt\" # note this was trained on a mac in cpu mode without cuda and mps\n",
    "args = TrainArgs(\n",
    "    num_digits=4,\n",
    "    trainset_size=100_000,\n",
    "    valset_size=5_000,\n",
    "    epochs=100,\n",
    "    batch_size=512,\n",
    "    lr_start=2e-3,\n",
    "    lr_end=1e-4,\n",
    "    # weight_decay=0.001, # not weight decay, could add this back in \n",
    "    weight_decay=0.00,\n",
    "    seed=42,\n",
    "    d_model=48,\n",
    "    d_head=24,\n",
    "    n_layers=2,\n",
    "    n_heads=3,\n",
    "    d_mlp=None,\n",
    "    normalization_type=\"LN\",\n",
    "    use_wandb=True,\n",
    "    device=device,\n",
    ")\n",
    "model = create_model(\n",
    "    num_digits=4,\n",
    "    seed=0,\n",
    "    d_model=48,\n",
    "    d_head=24,\n",
    "    n_layers=2,\n",
    "    n_heads=3,\n",
    "    normalization_type=\"LN\",\n",
    "    d_mlp=None,\n",
    "    device=device\n",
    ")\n",
    "model.load_state_dict(t.load(filename))\n",
    "\n",
    "# model = train(args)\n",
    "# t.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class ClusterState():\n",
    "    def __init__(self, model, dataset, min_samples=5, min_cluster_size=5, pred_i : List[int] = [10,11,12,13]  ):\n",
    "        self.model   = model\n",
    "        self.dataset = dataset\n",
    "        self.layers  = model.cfg.n_layers\n",
    "        self.heads   = model.cfg.n_heads\n",
    "        self.words   = model.cfg.n_ctx\n",
    "        self.ndims   = model.cfg.d_model\n",
    "        self.attn_only = model.cfg.attn_only\n",
    "        self.pred_i  = pred_i\n",
    "        self.result = {}\n",
    "\n",
    "        print(f\"Dataset size={dataset.shape[0]} examples\")\n",
    "        print(f\"Model has {self.layers} layers, {self.heads} heads, {self.words} words, {self.ndims} dimensions\")\n",
    "\n",
    "        model.reset_hooks()\n",
    "        self.ctrl_logits,self.ctrl_cache  = model.run_with_cache(dataset)\n",
    "        self.ctrl_preds = self.ctrl_logits.softmax(dim=-1).argmax(dim=-1)\n",
    "        model.reset_hooks()\n",
    "\n",
    "        self.filtered_words = []\n",
    "        for layer in range(self.layers) :\n",
    "            self.filtered_words.append( self.filter_words(layer) )\n",
    "        self.filtered_words.append( self.pred_i )\n",
    "\n",
    "        self.index = self.step_through_calc( self.create_index )\n",
    "\n",
    "    \n",
    "    def filter_words(self, layer, activation_name = \"resid_pre\") :\n",
    "        return [word for word in range(self.words) if self.substitute_centoids(activation_name, layer, word=word, mean=True).sum().item() > 0]\n",
    "\n",
    "    def step_through_calc(self, method ) :\n",
    "\n",
    "        def update_index(index, activation_name, layer, head, words) :\n",
    "            for word in words :\n",
    "                x = (activation_name, layer, head, word)\n",
    "                index[x] = method(*x)    \n",
    "\n",
    "        index = {}\n",
    "        for layer in range(self.layers):\n",
    "            for head in range(self.heads) :\n",
    "                # update_index(index, \"v\", layer, head, self.filtered_words[layer] )\n",
    "                update_index(index, \"pattern-q\", layer, head, self.filtered_words[layer+1] )\n",
    "                update_index(index, \"result\"   , layer, head, self.filtered_words[layer+1] )\n",
    "            update_index(index, \"attn_out\", layer, None, self.filtered_words[layer+1] )\n",
    "        return index\n",
    "\n",
    "\n",
    "    def create_index(self, *args) :\n",
    "        embeddings = self.embeddings(*args)\n",
    "        index = faiss.IndexFlatL2(embeddings.shape[1])  # Use IndexFlatL2 for L2 distance\n",
    "        index.add(embeddings.float())\n",
    "        return index\n",
    "    \n",
    "    def search_index(self, search_dataset , top_k):\n",
    "        model.reset_hooks()\n",
    "        logits, cache  = model.run_with_cache(search_dataset)\n",
    "        model.reset_hooks()\n",
    "        def _search_index(*args):\n",
    "            points = self.embeddings(*args, cache=cache)\n",
    "            distances, indices = self.index[args].search( points , top_k )\n",
    "            return indices.squeeze()\n",
    "        return _search_index\n",
    "\n",
    "    def pattern_words(self, pattern, threshold=.1 ) :\n",
    "        return t.where(  pattern.max(dim=0).values > threshold )[0].tolist()\n",
    "    \n",
    "\n",
    "    def all_the_same(self, items):\n",
    "        if not isinstance(items, t.Tensor):\n",
    "            items = t.tensor(items)\n",
    "        if items.eq(items[0]).all() :\n",
    "            return True , f\"{items[0].item()}\"\n",
    "        else :\n",
    "            return False, items.detach().cpu().numpy()\n",
    "    \n",
    "    def find_similatities(self, results) :\n",
    "\n",
    "        def _similatities(*args) :            \n",
    "            close_points = results[args]\n",
    "\n",
    "            scp  = [ [dataset.vocab[tok] for tok in dataset[point] ] for point in close_points ]\n",
    "\n",
    "            words = [1,2,3,4,6,7,8,9,11,12,13,14]\n",
    "\n",
    "            explanation = []\n",
    "            if ( args[0] == \"attn_out\" and args[1]==self.layers-1) :\n",
    "                words = [args[3]+1]     \n",
    "\n",
    "            for word in words :\n",
    "                print_it, result  = self.all_the_same(dataset[close_points,word])\n",
    "                if (print_it) :\n",
    "                    for point in range(len(scp)) :\n",
    "                        scp[point][word] = ( scp[point][word] , \"\" )\n",
    "                    explanation.append( f\"{dataset.format_word(word)} = {result}\" )\n",
    "            for i, word in enumerate(self.pred_i) :\n",
    "                w = word +1\n",
    "                if i > 0 :\n",
    "                    carry = [ Pairs.is_carry( dataset.p[point].item() , i  ) for point in close_points ]\n",
    "                    print_it, result  = self.all_the_same(carry)\n",
    "                    if (print_it) :\n",
    "                        if ( len(scp[0][w]) > 1 ) :\n",
    "                            for point in range(len(scp)) :\n",
    "                                scp[point][w] = ( scp[point][w][0] , f\"carry {result}\" ) \n",
    "                        else:\n",
    "                            for point in range(len(scp)) :      \n",
    "                                scp[point][w] = ( scp[point][w]    , f\"carry {result}\" ) \n",
    "\n",
    "                        explanation.append( f\"{dataset.format_word(w)} carry {result}\" )\n",
    "\n",
    "\n",
    "            return ( '\\n'.join(explanation) , scp )\n",
    "\n",
    "        return  _similatities          \n",
    "\n",
    "    def dependency(self) :\n",
    "        def _dependency(*args) :            \n",
    "            dependency = []\n",
    "            if ( args[0] == \"attn_out\" ) :\n",
    "                for head in range(self.heads) :\n",
    "                    dependency.append( (\"result\", args[1], head, args[3]) )\n",
    "            elif ( args[0] == \"result\" ) :\n",
    "                dependency.append( (\"pattern-q\", args[1], args[2], args[3]) )\n",
    "            elif ( args[0] == \"pattern-q\" and args[1]>0 ) :\n",
    "                embed   = self.embeddings(*args) \n",
    "                pwords  = self.pattern_words( embed )\n",
    "                fwords  = self.filtered_words[args[1]]\n",
    "                intersection = sorted(list(set(pwords) & set(fwords)))\n",
    "                for i in intersection :\n",
    "                    dependency.append( (\"attn_out\", args[1]-1, None, i) ) \n",
    "            return dependency\n",
    "        return _dependency\n",
    "    \n",
    "    def embeddings(self, activation_name, layer, head, word, cache=None):\n",
    "        if (cache is None) :\n",
    "            cache = self.ctrl_cache \n",
    "\n",
    "        an  = activation_name.split(\"-\", 1)\n",
    "        if (len(an) == 2) :\n",
    "            activation = cache[ an[0], layer, an[1] ]\n",
    "        else :\n",
    "            activation = cache[ an[0], layer]\n",
    "            \n",
    "        if (an[0] == \"pattern\" or an[0] ==  \"attn_scores\") :\n",
    "            if   (an[1] == \"q\") :\n",
    "                return activation[:,head,word,:]   \n",
    "            elif (an[1] == \"v\") :\n",
    "                return activation[:,head,:,word] \n",
    "        elif (word is not None and head is not None) : \n",
    "            return activation[:, word, head, :] \n",
    "        elif (word is not None) :\n",
    "            return activation[:, word, :] \n",
    "        else :\n",
    "            return activation \n",
    "        \n",
    "    def _wh_hook(self, activation,hook,an,head,word):\n",
    "\n",
    "        if (an[0] == \"pattern\" or an[0] ==  \"attn_scores\") :\n",
    "            if   (an[1] == \"q\") :\n",
    "                activation[:,head,word,:] = activation[:,head,word,:].mean(0)   \n",
    "            elif (an[1] == \"v\") :\n",
    "                activation[:,head,:,word] = activation[:,head,:,word].mean(0)   \n",
    "        elif (word is not None and head is not None) : \n",
    "            activation[:, word, head, :] = activation[:, word, head, :].mean(0) \n",
    "        elif (word is not None) :\n",
    "            activation[:, word, :] = activation[:, word, :].mean(0) \n",
    "        else :\n",
    "            activation = activation.mean(0) \n",
    "\n",
    "        return activation\n",
    "\n",
    "    def wh_hook(self, an=None, head=None, word = None) :\n",
    "        return functools.partial(self._wh_hook, an=an, head=head, word=word )\n",
    "\n",
    "    def substitute_centoids(self, activation_name, layer, word=None, head=None, selection=slice(None), mean=False) :\n",
    "        self.model.reset_hooks()\n",
    "\n",
    "        an  = activation_name.split(\"-\", 1)\n",
    "        if (len(an) == 2) :\n",
    "            self.model.add_hook( utils.get_act_name(an[0], layer, an[1]), self.wh_hook( an,head,word)  )\n",
    "        else :\n",
    "            self.model.add_hook( utils.get_act_name(an[0], layer       ), self.wh_hook( an,head,word)  )\n",
    "\n",
    "        logits, cache = model.run_with_cache(self.dataset[selection,:])\n",
    "        self.model.reset_hooks()\n",
    "        preds = logits.softmax(dim=-1).argmax(dim=-1)\n",
    "\n",
    "        err = t.zeros((preds.shape[0]), dtype=t.bool)\n",
    "        for outcome_word in self.pred_i:\n",
    "            err = err | self.errors(preds, outcome_word, selection)\n",
    "\n",
    "        return err\n",
    "    \n",
    "    def errors(self, preds, outcome_word, selection=slice(None)) :\n",
    "        return self.ctrl_preds[selection,outcome_word] != preds[selection,outcome_word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size=3000 examples\n",
      "Model has 2 layers, 3 heads, 15 words, 48 dimensions\n"
     ]
    }
   ],
   "source": [
    "cluster_state = ClusterState(model,dataset.toks )\n",
    "dependency    =  cluster_state.step_through_calc( cluster_state.dependency() )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def create_examples(exs) :\n",
    "\n",
    "    def create_example(ex) :\n",
    "        example = [ dataset.vocab[tok] for tok in dataset.toks[ex] ] \n",
    "        carry   = [ Pairs.is_carry( dataset.p[ex] , i) for i,p in enumerate(cluster_state.pred_i) ]\n",
    "        for i, word in enumerate(cluster_state.pred_i) :\n",
    "            w = word +1   \n",
    "            if i >0 :\n",
    "                example[w] = ( example[w] , f\"carry {carry[i]}\" ) \n",
    "        return example  \n",
    "\n",
    "    examples = []\n",
    "    for ex in range(exs) :\n",
    "        example      = create_example(ex)\n",
    "        results      = cluster_state.step_through_calc( cluster_state.search_index( dataset.toks[ex], 10 ) )\n",
    "        similarities = cluster_state.step_through_calc( cluster_state.find_similatities( results ) )\n",
    "        examples.append( (example, similarities, dependency) )\n",
    "\n",
    "    with open( f\"visualization/examples.pkl\", 'wb') as file:\n",
    "        pickle.dump(examples, file)\n",
    "\n",
    "create_examples(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def recursive_print( start , depth=2) : \n",
    "#     tab = \" \" * depth\n",
    "#     print(tab + str(start) )\n",
    "#     print(tab + str(similarities[start][0]) )\n",
    "#     for d in dependency[start] :\n",
    "#         recursive_print( d , depth=depth+2 )\n",
    "\n",
    "# recursive_print( start[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ST 3190 + 4577 = 7767\n",
      "[0 0 2 0]\n",
      "[0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print( dataset.format(0))\n",
    "print( Pairs.p[ dataset.p[0] ])\n",
    "print( [ Pairs.is_carry( dataset.p[0].item() , i  ) for i,p in enumerate(cluster_state.pred_i) ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def logistic_regression(X,y) :\n",
    "    # Load the Iris dataset\n",
    "    # iris = load_iris()\n",
    "    # X = iris.data  # Features: sepal length, sepal width, petal length, petal width\n",
    "    # print(X.shape)\n",
    "    # y = iris.target  # Target: species of Iris (setosa, versicolor, virginica)\n",
    "    # print(y.shape)\n",
    "    # print(y)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # It's a good practice to standardize the data (mean=0 and variance=1)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Create the logistic regression model\n",
    "    log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    log_reg.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = log_reg.predict(X_test)\n",
    "\n",
    "    # Output the accuracy of the model\n",
    "    print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n",
    "\n",
    "    # Output the confusion matrix\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "def linear_regression(X, y):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # It's a good practice to standardize the data (mean=0 and variance=1)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Create the linear regression model\n",
    "    lin_reg = LinearRegression()\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = lin_reg.predict(X_test)\n",
    "\n",
    "    # Output the Mean Squared Error of the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f'Mean Squared Error: {mse:.2f}')\n",
    "\n",
    "    # Output the R-squared score of the model\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f'R-squared: {r2:.2f}')\n",
    "\n",
    "    return lin_reg  # Optional: return the model for further use\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def pca_the_points(points, n_components=10, graph=True,labels=None):\n",
    "\n",
    "    n_components=min(n_components,points.size(0))\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    data = points.detach().cpu().numpy()\n",
    "    pca_result = pca.fit_transform(data)\n",
    "\n",
    "    # Get the two components\n",
    "    comp1 = pca.components_[0]\n",
    "    comp2 = pca.components_[1]\n",
    "\n",
    "    # Print the explained variance by each component\n",
    "    print(\"Explained variance by component:\", np.round(pca.explained_variance_ratio_,2)*100)\n",
    "\n",
    "    if graph:\n",
    "        # Plotting the explained variance\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.bar(range(n_components), pca.explained_variance_ratio_, align='center')\n",
    "        plt.xlabel(\"Principal Component\")\n",
    "        plt.ylabel(\"Explained Variance Ratio\")\n",
    "        plt.title(\"Explained Variance by Principal Component\")\n",
    "        plt.show()\n",
    "\n",
    "     # Create a bar chart\n",
    "        width = 0.4\n",
    "        indices = np.arange(48)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bar1 = plt.bar(indices, pca.components_[0], width, color='b', label='Principal Component 1')\n",
    "        bar2 = plt.bar(indices + width, pca.components_[1], width, color='r', label='Principal Component 2')\n",
    "        plt.xlabel(\"Dimensions\")\n",
    "        plt.ylabel(\"Weight\")\n",
    "        plt.title(\"Weights of Dimensions for First Two Principal Components\")\n",
    "        plt.xticks(indices + width / 2, indices)  # X-axis labels (centered)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        n=points.size(0) # n=pca_result.shape[0] number of points to plot\n",
    "\n",
    "        xi = 1\n",
    "        yi = 2\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.scatter(pca_result[:n, xi], pca_result[:n, yi], s=50, c='blue', edgecolors='k', marker='o', alpha=0.7)\n",
    "\n",
    "        if labels is  None :\n",
    "            for i in range(n) :\n",
    "                plt.annotate(i, (pca_result[i, xi], pca_result[i, yi]), fontsize=18, ha=\"right\")\n",
    "        else :\n",
    "            for i,label in enumerate(labels) :\n",
    "                plt.annotate( label , (pca_result[i, xi], pca_result[i, yi]), fontsize=18, ha=\"right\")\n",
    "\n",
    "        # Setting labels and title\n",
    "        plt.xlabel(f\"Principal Component {xi}\")\n",
    "        plt.ylabel(f\"Principal Component {yi}\")\n",
    "        # plt.xlim(-.7, .7)  # Set the x-axis limits \n",
    "        # plt.ylim(-.7, .7)  # Set the y-axis limits\n",
    "        plt.title(f\"Projection of Vectors on PC {xi} and PC {yi}\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    return comp1, comp2\n",
    "\n",
    "def show_gram_matrix(y, y_pred) :\n",
    "\n",
    "\n",
    "    y_l2_normalized      = F.normalize(y, p=2, dim=0)\n",
    "    y_pred_l2_normalized = F.normalize(y_pred, p=2, dim=0)\n",
    "\n",
    "    gram_matrix = t.mm( y_l2_normalized, y_pred_l2_normalized.t())\n",
    "\n",
    "    colorscale = [[0, 'white'], [.5, 'white'], [1.0, 'green']]\n",
    "\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=gram_matrix.detach().cpu().numpy(),\n",
    "        colorscale=colorscale,\n",
    "        zmin=-1,\n",
    "        zmax=1,\n",
    "        x=list(range(0, gram_matrix.size(-1))),  \n",
    "        y=list(range(0, gram_matrix.size(-1))),\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Gram Matrix Predicted vs Actual (correlation)',\n",
    "        xaxis_title='Actual Embedding position p-1',\n",
    "        yaxis_title='Predicted Embedding at position p-1',\n",
    "        yaxis_autorange='reversed', \n",
    "        autosize=False,\n",
    "        height=500,\n",
    "        width=500,\n",
    "    )\n",
    "    print(\"predicted embedding correlates highest with actual embedding\")\n",
    "    fig.show(\"png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()\n",
    "logits,cache  = model.run_with_cache(dataset.toks)\n",
    "preds =logits.softmax(dim=-1).argmax(dim=-1)\n",
    "model.reset_hooks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /Users/alistair/opt/anaconda3/envs/arena/lib/python3.8/site-packages (1.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting digit 10\n",
      "at word 10, Logistic Regression predicts carrying the 11 digit\n",
      "Accuracy: 0.98\n",
      "Confusion Matrix:\n",
      "[[217   0   0   0]\n",
      " [  0  67   1   1]\n",
      " [  0   0   9   7]\n",
      " [  0   0   0 298]]\n"
     ]
    }
   ],
   "source": [
    "layer = 1\n",
    "digit = len(dataset.toks[0]) - 1 - 4\n",
    "output_digit = digit+1\n",
    "print(f\"Predicting digit {digit}\")\n",
    "\n",
    "y_orig = dataset.p.detach().cpu().numpy()\n",
    "remap = {}\n",
    "for i in range(Pairs.p.shape[0]) :\n",
    "    p = Pairs.p[i]\n",
    "    if   p[1] == 2 :\n",
    "        remap[i] = 0\n",
    "    elif p[1] == 1 and p[2]==2 :\n",
    "        remap[i] = 1\n",
    "    elif p[1] == 1 and p[2]==1 and p[3]==2:\n",
    "        remap[i] = 2\n",
    "    else :\n",
    "        remap[i] = 3\n",
    "\n",
    "seq   = 10\n",
    "y  = np.array([ remap[y_orig[i]] for i in range(y_orig.shape[0]) ])\n",
    "X = cache[\"q\",layer][:,seq,:,:].sum(1,keepdim=False)\n",
    "print(f\"at word {seq}, Logistic Regression predicts carrying the {digit+1} digit\")\n",
    "logistic_regression(X.detach().cpu().numpy(),y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_index(embeddings):\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])  # Use IndexFlatL2 for L2 distance\n",
    "    index.add(embeddings.float())\n",
    "    return index\n",
    "\n",
    "def search_index(index, points, k):\n",
    "    distances, indices = index.search(points, k + 1)\n",
    "    # return indices[:, 1:].squeeze()\n",
    "    return indices.squeeze()\n",
    "\n",
    "def print_points( indices,print_carry=True, index=0  ) :\n",
    "\n",
    "    print( f\"Set {index}:\" )\n",
    "    for index in indices :\n",
    "\n",
    "        if print_carry :\n",
    "            p = Pairs.p[dataset.p[index].item()]\n",
    "            if p[1] == 2 or (p[1]==1 and p[2]==2) or (p[1]==1 and p[2]==1 and p[3]==2) :\n",
    "                print( \"        [  carry ]\" , dataset.format(index) )\n",
    "            else :\n",
    "                print( \"        [no carry]\" , dataset.format(index) )\n",
    "        else :\n",
    "            print( dataset.format(index)[5:9] )\n",
    "\n",
    "\n",
    "\n",
    "layer = 0\n",
    "head = 0\n",
    "activation = \"q\"\n",
    "word = 6\n",
    "X = cache[activation,layer][:,word,head,:]#.sum(1,keepdim=False)\n",
    "index = create_index(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 0:\n",
      "        [no carry] ST 3190 + 4577 = 7767\n",
      "        [no carry] ST 3191 + 4599 = 7790\n",
      "        [no carry] ST 4484 + 4465 = 8949\n",
      "        [no carry] ST 4510 + 4142 = 8652\n",
      "        [no carry] ST 0315 + 4143 = 4458\n",
      "        [  carry ] ST 2729 + 4986 = 7715\n",
      "        [no carry] ST 4613 + 4361 = 8974\n",
      "        [  carry ] ST 4748 + 4641 = 9389\n",
      "        [  carry ] ST 2789 + 4332 = 7121\n",
      "        [no carry] ST 4140 + 4755 = 8895\n",
      "        [  carry ] ST 2795 + 4604 = 7399\n",
      "\n",
      "Set 1:\n",
      "        [  carry ] ST 5625 + 3755 = 9380\n",
      "        [  carry ] ST 0854 + 3188 = 4042\n",
      "        [no carry] ST 2191 + 3698 = 5889\n",
      "        [no carry] ST 5963 + 3013 = 8976\n",
      "        [no carry] ST 1245 + 3713 = 4958\n",
      "        [  carry ] ST 5936 + 3963 = 9899\n",
      "        [  carry ] ST 5813 + 3692 = 9505\n",
      "        [  carry ] ST 1925 + 3593 = 5518\n",
      "        [no carry] ST 4050 + 3549 = 7599\n",
      "        [no carry] ST 2377 + 3622 = 5999\n",
      "        [no carry] ST 3402 + 3241 = 6643\n",
      "\n",
      "Set 2:\n",
      "        [no carry] ST 2457 + 6502 = 8959\n",
      "        [no carry] ST 0286 + 6313 = 6599\n",
      "        [  carry ] ST 2989 + 6019 = 9008\n",
      "        [  carry ] ST 0470 + 6850 = 7320\n",
      "        [  carry ] ST 1222 + 6926 = 8148\n",
      "        [no carry] ST 2476 + 6133 = 8609\n",
      "        [no carry] ST 1152 + 6467 = 7619\n",
      "        [  carry ] ST 1207 + 6899 = 8106\n",
      "        [no carry] ST 2488 + 6489 = 8977\n",
      "        [  carry ] ST 1388 + 6881 = 8269\n",
      "        [  carry ] ST 0893 + 6196 = 7089\n",
      "\n",
      "Set 3:\n",
      "        [  carry ] ST 0742 + 2358 = 3100\n",
      "        [  carry ] ST 4529 + 2648 = 7177\n",
      "        [  carry ] ST 3971 + 2696 = 6667\n",
      "        [  carry ] ST 6353 + 2744 = 9097\n",
      "        [no carry] ST 6721 + 2165 = 8886\n",
      "        [  carry ] ST 1902 + 2317 = 4219\n",
      "        [  carry ] ST 2806 + 2561 = 5367\n",
      "        [  carry ] ST 4731 + 2561 = 7292\n",
      "        [no carry] ST 0181 + 2762 = 2943\n",
      "        [no carry] ST 3609 + 2210 = 5819\n",
      "        [no carry] ST 6202 + 2791 = 8993\n",
      "\n",
      "Set 4:\n",
      "        [  carry ] ST 7852 + 1348 = 9200\n",
      "        [  carry ] ST 7632 + 1369 = 9001\n",
      "        [  carry ] ST 0676 + 1893 = 2569\n",
      "        [  carry ] ST 3148 + 1899 = 5047\n",
      "        [  carry ] ST 4384 + 1635 = 6019\n",
      "        [  carry ] ST 4879 + 1555 = 6434\n",
      "        [  carry ] ST 5060 + 1959 = 7019\n",
      "        [no carry] ST 0324 + 1355 = 1679\n",
      "        [  carry ] ST 4386 + 1953 = 6339\n",
      "        [no carry] ST 1152 + 1746 = 2898\n",
      "        [no carry] ST 1332 + 1305 = 2637\n",
      "\n",
      "Set 0:\n",
      "90 +\n",
      "91 +\n",
      "84 +\n",
      "10 +\n",
      "15 +\n",
      "29 +\n",
      "13 +\n",
      "48 +\n",
      "89 +\n",
      "40 +\n",
      "95 +\n",
      "\n",
      "Set 1:\n",
      "25 +\n",
      "54 +\n",
      "91 +\n",
      "63 +\n",
      "45 +\n",
      "36 +\n",
      "13 +\n",
      "25 +\n",
      "50 +\n",
      "77 +\n",
      "02 +\n",
      "\n",
      "Set 2:\n",
      "57 +\n",
      "86 +\n",
      "89 +\n",
      "70 +\n",
      "22 +\n",
      "76 +\n",
      "52 +\n",
      "07 +\n",
      "88 +\n",
      "88 +\n",
      "93 +\n",
      "\n",
      "Set 3:\n",
      "42 +\n",
      "29 +\n",
      "71 +\n",
      "53 +\n",
      "21 +\n",
      "02 +\n",
      "06 +\n",
      "31 +\n",
      "81 +\n",
      "09 +\n",
      "02 +\n",
      "\n",
      "Set 4:\n",
      "52 +\n",
      "32 +\n",
      "76 +\n",
      "48 +\n",
      "84 +\n",
      "79 +\n",
      "60 +\n",
      "24 +\n",
      "86 +\n",
      "52 +\n",
      "32 +\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5) :\n",
    "    indices = search_index(index, X[i].unsqueeze(0), 10)\n",
    "    print_points(indices, index=i)\n",
    "    print(\"\")\n",
    "\n",
    "for i in range(5) :\n",
    "    indices = search_index(index, X[i].unsqueeze(0), 10)\n",
    "    print_points(indices, print_carry=False, index=i )\n",
    "    print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apXYle', 'baXYana', 'chXYrry']\n"
     ]
    }
   ],
   "source": [
    "my_list = [\"apple\", \"banana\", \"cherry\"]\n",
    "pos = 2\n",
    "my_tuple = ('X', 'Y')\n",
    "\n",
    "# Function to replace character at pos with my_tuple\n",
    "def replace_with_tuple(s, pos, t):\n",
    "    return s[:pos] + ''.join(t) + s[pos + 1:]\n",
    "\n",
    "# Applying the function to each element in the list\n",
    "new_list = [replace_with_tuple(s, pos, my_tuple) for s in my_list]\n",
    "\n",
    "print(new_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tl_intro_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
