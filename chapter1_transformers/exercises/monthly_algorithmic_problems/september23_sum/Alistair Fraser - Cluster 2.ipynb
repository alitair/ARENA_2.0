{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "import os;\n",
    "\n",
    "os.environ[\"ACCELERATE_ENABLE_RICH\"] = \"0\"\n",
    "\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = os.path.basename(globals()['__vsc_ipynb_file__']) \n",
    "\n",
    "# os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "\n",
    "import sys\n",
    "from functools import partial\n",
    "import json\n",
    "from typing import List, Tuple, Union, Optional, Callable, Dict\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import circuitsvis as cv\n",
    "import webbrowser\n",
    "from IPython.display import display\n",
    "from transformer_lens import utils, ActivationCache, HookedTransformer, HookedTransformerConfig\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens.components import LayerNorm\n",
    "from eindex import eindex\n",
    "\n",
    "\n",
    "from sklearn.cluster import DBSCAN, HDBSCAN, OPTICS\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# t.set_grad_enabled(False)\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = r\"chapter1_transformers\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = exercises_dir / \"monthly_algorithmic_problems\" / \"september23_sum\"\n",
    "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "from monthly_algorithmic_problems.september23_sum.model import create_model\n",
    "from monthly_algorithmic_problems.september23_sum.training import train, TrainArgs\n",
    "from monthly_algorithmic_problems.september23_sum.dataset import SumDataset,Pairs\n",
    "from plotly_utils import hist, bar, imshow\n",
    "\n",
    "# Running this on a macbook air and mps is flaky\n",
    "device = t.device(\"cpu\") #t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "-- Note: I rewrote the Dataset class so that it balances no carry, plain carry and cascading carry  classes evenly. this puts a larger emphasis difficult cases relative to the original way this dataset was written. With random additions, cascading carry is an edge case that is quite infrequent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SumDataset(size=1000, num_digits=4).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "-- Note: I removed weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = section_dir / \"sum_model_normal.pt\" # note this was trained on a mac in cpu mode without cuda and mps\n",
    "args = TrainArgs(\n",
    "    num_digits=4,\n",
    "    trainset_size=100_000,\n",
    "    valset_size=5_000,\n",
    "    epochs=100,\n",
    "    batch_size=512,\n",
    "    lr_start=2e-3,\n",
    "    lr_end=1e-4,\n",
    "    # weight_decay=0.001, # not weight decay, could add this back in \n",
    "    weight_decay=0.00,\n",
    "    seed=42,\n",
    "    d_model=48,\n",
    "    d_head=24,\n",
    "    n_layers=2,\n",
    "    n_heads=3,\n",
    "    d_mlp=None,\n",
    "    normalization_type=\"LN\",\n",
    "    use_wandb=True,\n",
    "    device=device,\n",
    ")\n",
    "model = create_model(\n",
    "    num_digits=4,\n",
    "    seed=0,\n",
    "    d_model=48,\n",
    "    d_head=24,\n",
    "    n_layers=2,\n",
    "    n_heads=3,\n",
    "    normalization_type=\"LN\",\n",
    "    d_mlp=None,\n",
    "    device=device\n",
    ")\n",
    "model.load_state_dict(t.load(filename))\n",
    "\n",
    "# model = train(args)\n",
    "# t.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "\n",
    "class ClusterState():\n",
    "    def __init__(self, model, dataset, min_samples=5, min_cluster_size=5, pred_i : List[int] = [10,11,12,13]  ):\n",
    "        self.model   = model\n",
    "        self.dataset = dataset\n",
    "        self.layers  = model.cfg.n_layers\n",
    "        self.heads   = model.cfg.n_heads\n",
    "        self.words   = model.cfg.n_ctx\n",
    "        self.ndims   = model.cfg.d_model\n",
    "        self.attn_only = model.cfg.attn_only\n",
    "        self.pred_i  = pred_i\n",
    "        self.min_samples      = min_samples\n",
    "        self.min_cluster_size = min_cluster_size\n",
    "        self.result = {}\n",
    "\n",
    "        print(f\"Dataset size={dataset.shape[0]} examples\")\n",
    "        print(f\"Model has {self.layers} layers, {self.heads} heads, {self.words} words, {self.ndims} dimensions\")\n",
    "\n",
    "        model.reset_hooks()\n",
    "        self.ctrl_logits,self.ctrl_cache  = model.run_with_cache(dataset)\n",
    "        self.ctrl_preds = self.ctrl_logits.softmax(dim=-1).argmax(dim=-1)\n",
    "        model.reset_hooks()\n",
    "\n",
    "        self.filtered_words = []\n",
    "        for layer in range(self.layers) :\n",
    "            self.filtered_words.append( self.filter_words(layer) )\n",
    "        self.filtered_words.append( self.pred_i )\n",
    "\n",
    "        previous_steps = None\n",
    "        for layer in range(self.layers):\n",
    "            print(f\"layer={layer} \")\n",
    "\n",
    "            for head in range(self.heads) :\n",
    "                print(f\"  head={head}\")\n",
    "\n",
    "                activation_name, previous_steps, words  = \"v\",   [[\"attn_out\" , layer-1]] if layer>0 else None, self.filtered_words[layer]\n",
    "                print(f\"    activation_name={activation_name} words={words}\")\n",
    "                for word in words :\n",
    "                    self.calc(activation_name, layer, head, word, previous_steps)    \n",
    "                \n",
    "                activation_name, previous_steps, words =   \"pattern-q\", [ [\"v\" , layer] ], self.filtered_words[layer+1] \n",
    "                print(f\"    activation_name={activation_name} words={words}\")\n",
    "                for word in words :\n",
    "                    self.calc(activation_name, layer, head, word, previous_steps) \n",
    "\n",
    "            activation_name, previous_steps, words =   \"attn_out\", [ [\"pattern-q\" , layer] ], self.filtered_words[layer+1] \n",
    "            print( f\"  activation_name={'attn_out'} words={words} \")\n",
    "            for word in words :\n",
    "                self.calc(activation_name, layer, None, word, previous_steps) \n",
    "\n",
    "\n",
    "    def optimize_clustering(self, cluster_function) :\n",
    "\n",
    "        min_err_c = len(dataset) + 1\n",
    "        for min_samples in [2,3,5,7]:\n",
    "            for min_cluster_size in [5,10,15,20] :\n",
    "                self.min_samples      = min_samples\n",
    "                self.min_cluster_size = min_cluster_size\n",
    "                err, err_c, cluster_labels = cluster_function()\n",
    "                if (err_c < min_err_c) :\n",
    "                    min_err_c  = err_c\n",
    "                    min_cluster_labels   = cluster_labels\n",
    "                    min_err  = err\n",
    "                if (err_c == 0 ) :\n",
    "                    break\n",
    "        return min_err, min_err_c, min_cluster_labels\n",
    "\n",
    "                    \n",
    "    def calc(self, activation_name, layer, head, word, previous_steps) :\n",
    "\n",
    "        def cluster_function() :\n",
    "            self.result[ (activation_name,layer, head, word) ] = None\n",
    "            err    = self.substitute_centoids(activation_name, layer, word=word ,head=head , previous_steps=previous_steps )\n",
    "            errs_c = err.sum().item()\n",
    "            cluster_labels = self.result[ (activation_name,layer, head, word) ]\n",
    "            return err, errs_c, cluster_labels\n",
    "\n",
    "        err, errs_c, cluster_labels = self.optimize_clustering(cluster_function)\n",
    "        n_clusters = self.calc_n_clusters( cluster_labels )\n",
    "\n",
    "        if (errs_c > 0) :\n",
    "\n",
    "            patch_cluster_labels = t.arange(len(dataset)).int()\n",
    "\n",
    "            cluster_labels[ err ]   = -2\n",
    "            patch_cluster_labels[ ~err ]  = -2\n",
    "            patch_cluster_labels = self.combine_cluster_labels([cluster_labels, patch_cluster_labels])\n",
    "\n",
    "            n_clusters_matching = self.calc_n_clusters(patch_cluster_labels) \n",
    "\n",
    "            self.result[ (activation_name, layer, head, word) ] = patch_cluster_labels\n",
    "            err   = self.substitute_centoids(activation_name, layer, word=word ,head=head ,previous_steps=previous_steps )\n",
    "            errs_m = err.sum().item()\n",
    "\n",
    "            print(f\"      word={word} n_c={n_clusters} err_c={errs_c}, n_m={n_clusters_matching} err_m={errs_m}\" )\n",
    "        else :\n",
    "            print(f\"      word={word} n_c={n_clusters} err_c={errs_c}\" )\n",
    "\n",
    "    def calc_n_clusters(self, cluster_labels) :\n",
    "        return  cluster_labels.unique().shape[0]\n",
    "\n",
    "    def combine_cluster_labels(self, cluster_labels_list):\n",
    "        return t.stack(cluster_labels_list, dim=1).unique(dim=0, return_inverse=True)[1]\n",
    "\n",
    "    def pattern_words(self, pattern, threshold=.05 ) :\n",
    "        return t.where(  pattern.max(dim=0).values > threshold )[0].tolist()\n",
    "\n",
    "    def cluster_data(self, data, word) :\n",
    "        return self.cluster_values( word, data=data[0], activation_name=data[1], layer=data[2], head=data[3] )\n",
    "\n",
    "    def cluster_values(self, word, data=None, activation_name=None , layer=None, head=None, selection=slice(None) ) :\n",
    "        data = data[selection,word,:]\n",
    "        cluster_labels, silhouette_values, n_clusters, silhouette_avg  = self.find_clusters(data)\n",
    "        return cluster_labels, silhouette_values, n_clusters, silhouette_avg, self.substitute_centoids(layer, activation_name, word=word, head=head,selection=selection)\n",
    "\n",
    "    def filter_words(self, layer, activation_name = \"resid_pre\") :\n",
    "        return [word for word in range(self.words) if self.substitute_centoids(activation_name, layer, word=word, mean=True).sum().item() > 0]\n",
    "\n",
    "    def find_clusters(self, data):\n",
    "        data = data.detach().numpy()\n",
    "        dbscan = HDBSCAN(min_samples=self.min_samples, min_cluster_size=self.min_cluster_size)\n",
    "        cluster_labels = dbscan.fit_predict(data)\n",
    "        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "        \n",
    "        if n_clusters > 1:\n",
    "            silhouette_avg    = silhouette_score(data, cluster_labels)\n",
    "            silhouette_values = silhouette_samples(data, cluster_labels)\n",
    "        else:\n",
    "            silhouette_avg    = 0\n",
    "            silhouette_values = np.zeros(data.shape[0])\n",
    "\n",
    "        cluster_labels = t.from_numpy(cluster_labels).to(device)\n",
    "\n",
    "        return cluster_labels, silhouette_values, n_clusters, silhouette_avg\n",
    "    \n",
    "    def calc_centroid(self, data, mean=False,cluster_labels=None) :\n",
    "        if mean is True :\n",
    "            return data.mean(0), t.zeros(data.shape[0])\n",
    "        \n",
    "        if cluster_labels is None :\n",
    "            cluster_labels, silhouette_values, n_clusters, silhouette_avg = self.find_clusters(data)\n",
    "            # print( f\"    n_clusters={n_clusters} silhouette_avg={silhouette_avg:.2f}\")\n",
    "\n",
    "        unique_labels, inverse_indices = cluster_labels.unique(return_inverse=True)\n",
    "        centroids = t.vstack([data[cluster_labels == i].mean(0) for i in unique_labels])\n",
    "\n",
    "        replacement_value = centroids[inverse_indices]\n",
    "        # replacement_value[ cluster_labels < 0] = data[ cluster_labels < 0]\n",
    "        \n",
    "        return replacement_value, cluster_labels\n",
    "\n",
    "\n",
    "    def replace_hook(self,replace=None) :\n",
    "        _replace_hook = lambda activation,hook,replace : replace\n",
    "        return functools.partial(_replace_hook, replace=replace)\n",
    "\n",
    "    def _wh_hook(self, activation,hook,an,layer,head,word,mean):\n",
    "\n",
    "        a = an[0] if len(an) == 1 else an[0] + \"-\" + an[1]\n",
    "\n",
    "        cl = self.result.get( (a , layer, head, word), None) \n",
    "\n",
    "        if (an[0] == \"pattern\" or an[0] ==  \"attn_scores\") :\n",
    "            if   (an[1] == \"q\") :\n",
    "                activation[:,head,word,:], cl = self.calc_centroid(activation[:,head,word,:],mean=mean,cluster_labels=cl)     \n",
    "            elif (an[1] == \"v\") :\n",
    "                activation[:,head,:,word], cl = self.calc_centroid(activation[:,head,:,word],mean=mean,cluster_labels=cl)   \n",
    "        elif (word is not None and head is not None) : \n",
    "            activation[:, word, head, :], cl  = self.calc_centroid(activation[:, word, head, :],mean=mean,cluster_labels=cl) \n",
    "        elif (word is not None) :\n",
    "            activation[:, word, :], cl        = self.calc_centroid(activation[:, word, :],mean=mean,cluster_labels=cl) \n",
    "        else :\n",
    "            activation, cl = self.calc_centroid(activation,cluster_labels=cl) \n",
    "\n",
    "        self.result[ (a , layer, head, word) ] = cl\n",
    "\n",
    "        return activation\n",
    "    \n",
    "    def save_values(self,ctrl_cache, cache, word,head,an,selection=slice(None)):\n",
    "        if (an[0] == \"pattern\" or an[0] ==  \"attn_scores\") :\n",
    "            if   (an[1] == \"q\") :\n",
    "                ctrl_cache[selection,head,word,:] =cache[selection,head,word,:] \n",
    "            elif (an[1] == \"v\") :\n",
    "                ctrl_cache[selection,head,:,word] = cache[selection,head,:,word] \n",
    "        elif (word is not None and head is not None) : \n",
    "            ctrl_cache[ selection, word, head, :]=cache[ selection, word, head, :]\n",
    "        elif (word is not None) :\n",
    "            ctrl_cache[ selection, word, :]=cache[ selection, word, :]  \n",
    "        else :\n",
    "            ctrl_cache[ selection ]=cache[ selection ]\n",
    "\n",
    "    def wh_hook(self, an=None, layer=None, head=None, word = None, mean=False) :\n",
    "        return functools.partial(self._wh_hook, an=an, layer=layer, head=head, word=word, mean=mean)\n",
    "\n",
    "    def substitute_centoids(self, activation_name, layer, word=None, head=None, selection=slice(None), previous_steps=None, mean=False, cluster=True, save=True) :\n",
    "        self.model.reset_hooks()\n",
    "\n",
    "        previous_steps = None\n",
    "        if (previous_steps is not None and len(previous_steps)!=0) :\n",
    "            for step in previous_steps :      \n",
    "                pn = step[0].split(\"-\", 1)\n",
    "                previous_layer = step[1]\n",
    "                if (len(pn) == 2) :\n",
    "                    self.model.add_hook( utils.get_act_name(pn[0], previous_layer, pn[1]), self.replace_hook( self.ctrl_cache[ pn[0], previous_layer, pn[1] ] )  )\n",
    "                else :\n",
    "                    self.model.add_hook( utils.get_act_name(pn[0], previous_layer       ), self.replace_hook( self.ctrl_cache[ pn[0], previous_layer       ] )  )\n",
    "\n",
    "        an  = activation_name.split(\"-\", 1)\n",
    "        if (cluster) :\n",
    "            if (len(an) == 2) :\n",
    "                self.model.add_hook( utils.get_act_name(an[0], layer, an[1]), self.wh_hook( an,layer,head,word,mean)  )\n",
    "            else :\n",
    "                self.model.add_hook( utils.get_act_name(an[0], layer       ), self.wh_hook( an,layer,head,word,mean)  )\n",
    "\n",
    "        logits, cache = model.run_with_cache(self.dataset[selection,:])\n",
    "        self.model.reset_hooks()\n",
    "        preds = logits.softmax(dim=-1).argmax(dim=-1)\n",
    "\n",
    "        err   = self.cluster_errors(preds, self.result[ (activation_name,layer, head, word) ] , selection=selection)\n",
    "\n",
    "        if (save and err.sum().item() == 0) :\n",
    "            if (len(an) == 2) :\n",
    "                self.save_values( self.ctrl_cache[ an[0], layer, an[1] ], cache[ an[0], layer, an[1] ] , word, head, an)\n",
    "            else :\n",
    "                self.save_values( self.ctrl_cache[ an[0], layer], cache[ an[0], layer ] , word, head, an)\n",
    "\n",
    "        return err\n",
    "    \n",
    "    def errors(self, preds, outcome_word, selection=slice(None)) :\n",
    "        return self.ctrl_preds[selection,outcome_word] != preds[selection,outcome_word]\n",
    "    \n",
    "    def cluster_errors(self, preds, cluster_labels=None, selection=slice(None)):\n",
    "\n",
    "        err = t.zeros((preds.shape[0]), dtype=t.bool)\n",
    "        for outcome_word in self.pred_i:\n",
    "            err = err | self.errors(preds, outcome_word, selection)\n",
    "\n",
    "        for cluster in cluster_labels.unique():\n",
    "            cluster_members = (cluster_labels == cluster)\n",
    "            if err[cluster_members].any():\n",
    "                err[cluster_members] = True\n",
    "\n",
    "        return err\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size=1000 examples\n",
      "Model has 2 layers, 3 heads, 15 words, 48 dimensions\n",
      "layer=0 \n",
      "  head=0\n",
      "    activation_name=v words=[1, 2, 3, 4, 6, 7, 8, 9]\n",
      "      word=1 n_c=9 err_c=0\n",
      "      word=2 n_c=10 err_c=0\n",
      "      word=3 n_c=10 err_c=0\n",
      "      word=4 n_c=10 err_c=0\n",
      "      word=6 n_c=9 err_c=0\n",
      "      word=7 n_c=10 err_c=0\n",
      "      word=8 n_c=10 err_c=0\n",
      "      word=9 n_c=10 err_c=0\n",
      "    activation_name=pattern-q words=[1, 5, 6, 7, 9, 10, 11, 12, 13]\n",
      "      word=1 n_c=9 err_c=0\n",
      "      word=5 n_c=100 err_c=0\n",
      "      word=6 n_c=45 err_c=0\n",
      "      word=7 n_c=95 err_c=19, n_m=113 err_m=0\n",
      "      word=9 n_c=79 err_c=27, n_m=105 err_m=0\n",
      "      word=10 n_c=24 err_c=35, n_m=58 err_m=0\n",
      "      word=11 n_c=19 err_c=27, n_m=45 err_m=0\n",
      "      word=12 n_c=53 err_c=112, n_m=164 err_m=0\n",
      "      word=13 n_c=64 err_c=0\n",
      "  head=1\n",
      "    activation_name=v words=[1, 2, 3, 4, 6, 7, 8, 9]\n",
      "      word=1 n_c=9 err_c=0\n",
      "      word=2 n_c=10 err_c=0\n",
      "      word=3 n_c=10 err_c=0\n",
      "      word=4 n_c=10 err_c=0\n",
      "      word=6 n_c=9 err_c=0\n",
      "      word=7 n_c=10 err_c=0\n",
      "      word=8 n_c=10 err_c=0\n",
      "      word=9 n_c=10 err_c=0\n",
      "    activation_name=pattern-q words=[1, 5, 6, 7, 9, 10, 11, 12, 13]\n",
      "      word=1 n_c=9 err_c=0\n",
      "      word=5 n_c=112 err_c=0\n",
      "      word=6 n_c=32 err_c=0\n",
      "      word=7 n_c=25 err_c=81, n_m=105 err_m=0\n",
      "      word=9 n_c=15 err_c=44, n_m=58 err_m=0\n",
      "      word=10 n_c=39 err_c=602, n_m=632 err_m=0\n",
      "      word=11 n_c=23 err_c=71, n_m=93 err_m=0\n",
      "      word=12 n_c=102 err_c=106, n_m=207 err_m=0\n",
      "      word=13 n_c=70 err_c=0\n",
      "  head=2\n",
      "    activation_name=v words=[1, 2, 3, 4, 6, 7, 8, 9]\n",
      "      word=1 n_c=9 err_c=0\n",
      "      word=2 n_c=10 err_c=0\n",
      "      word=3 n_c=10 err_c=0\n",
      "      word=4 n_c=10 err_c=0\n",
      "      word=6 n_c=9 err_c=0\n",
      "      word=7 n_c=10 err_c=0\n",
      "      word=8 n_c=10 err_c=0\n",
      "      word=9 n_c=10 err_c=0\n",
      "    activation_name=pattern-q words=[1, 5, 6, 7, 9, 10, 11, 12, 13]\n",
      "      word=1 n_c=9 err_c=0\n",
      "      word=5 n_c=92 err_c=0\n",
      "      word=6 n_c=42 err_c=36, n_m=77 err_m=0\n",
      "      word=7 n_c=26 err_c=34, n_m=59 err_m=0\n",
      "      word=9 n_c=73 err_c=0\n",
      "      word=10 n_c=31 err_c=629, n_m=657 err_m=0\n",
      "      word=11 n_c=15 err_c=95, n_m=109 err_m=0\n",
      "      word=12 n_c=51 err_c=0\n",
      "      word=13 n_c=35 err_c=0\n",
      "  activation_name=attn_out words=[1, 5, 6, 7, 9, 10, 11, 12, 13] \n",
      "      word=1 n_c=9 err_c=0\n",
      "      word=5 n_c=21 err_c=0\n",
      "      word=6 n_c=50 err_c=27, n_m=76 err_m=0\n",
      "      word=7 n_c=97 err_c=42, n_m=137 err_m=0\n",
      "      word=9 n_c=88 err_c=89, n_m=169 err_m=0\n",
      "      word=10 n_c=55 err_c=498, n_m=550 err_m=0\n",
      "      word=11 n_c=108 err_c=265, n_m=358 err_m=0\n",
      "      word=12 n_c=24 err_c=852, n_m=866 err_m=0\n",
      "      word=13 n_c=10 err_c=0\n",
      "layer=1 \n",
      "  head=0\n",
      "    activation_name=v words=[1, 5, 6, 7, 9, 10, 11, 12, 13]\n",
      "      word=1 n_c=9 err_c=0\n",
      "      word=5 n_c=21 err_c=0\n",
      "      word=6 n_c=49 err_c=0\n",
      "      word=7 n_c=98 err_c=0\n",
      "      word=9 n_c=87 err_c=19, n_m=105 err_m=0\n",
      "      word=10 n_c=12 err_c=36, n_m=47 err_m=0\n",
      "      word=11 n_c=51 err_c=52, n_m=101 err_m=0\n",
      "      word=12 n_c=11 err_c=0\n",
      "      word=13 n_c=93 err_c=0\n",
      "    activation_name=pattern-q words=[10, 11, 12, 13]\n",
      "      word=10 n_c=5 err_c=328, n_m=332 err_m=0\n",
      "      word=11 n_c=67 err_c=362, n_m=428 err_m=0\n",
      "      word=12 n_c=34 err_c=198, n_m=231 err_m=0\n",
      "      word=13 n_c=27 err_c=169, n_m=195 err_m=0\n",
      "  head=1\n",
      "    activation_name=v words=[1, 5, 6, 7, 9, 10, 11, 12, 13]\n",
      "      word=1 n_c=9 err_c=0\n",
      "      word=5 n_c=82 err_c=0\n",
      "      word=6 n_c=50 err_c=0\n",
      "      word=7 n_c=73 err_c=15, n_m=87 err_m=0\n",
      "      word=9 n_c=87 err_c=22, n_m=108 err_m=0\n",
      "      word=10 n_c=58 err_c=0\n",
      "      word=11 n_c=104 err_c=89, n_m=191 err_m=0\n",
      "      word=12 n_c=2 err_c=0\n",
      "      word=13 n_c=93 err_c=0\n",
      "    activation_name=pattern-q words=[10, 11, 12, 13]\n",
      "      word=10 n_c=103 err_c=95, n_m=197 err_m=0\n",
      "      word=11 n_c=13 err_c=124, n_m=136 err_m=0\n",
      "      word=12 n_c=65 err_c=217, n_m=281 err_m=0\n",
      "      word=13 n_c=65 err_c=102, n_m=166 err_m=0\n",
      "  head=2\n",
      "    activation_name=v words=[1, 5, 6, 7, 9, 10, 11, 12, 13]\n",
      "      word=1 n_c=9 err_c=0\n",
      "      word=5 n_c=15 err_c=0\n",
      "      word=6 n_c=48 err_c=27, n_m=74 err_m=0\n",
      "      word=7 n_c=98 err_c=24, n_m=121 err_m=0\n",
      "      word=9 n_c=87 err_c=16, n_m=102 err_m=0\n",
      "      word=10 n_c=34 err_c=340, n_m=373 err_m=0\n",
      "      word=11 n_c=102 err_c=69, n_m=169 err_m=0\n",
      "      word=12 n_c=5 err_c=0\n",
      "      word=13 n_c=94 err_c=0\n",
      "    activation_name=pattern-q words=[10, 11, 12, 13]\n",
      "      word=10 n_c=5 err_c=95, n_m=98 err_m=0\n",
      "      word=11 n_c=87 err_c=175, n_m=261 err_m=0\n",
      "      word=12 n_c=16 err_c=188, n_m=203 err_m=0\n",
      "      word=13 n_c=55 err_c=105, n_m=159 err_m=0\n",
      "  activation_name=attn_out words=[10, 11, 12, 13] \n",
      "      word=10 n_c=86 err_c=95, n_m=179 err_m=0\n",
      "      word=11 n_c=78 err_c=115, n_m=190 err_m=0\n",
      "      word=12 n_c=70 err_c=38, n_m=106 err_m=0\n",
      "      word=13 n_c=78 err_c=26, n_m=103 err_m=0\n"
     ]
    }
   ],
   "source": [
    "cluster_state = ClusterState(model,dataset.toks,min_samples=4,min_cluster_size=10 ) #4,10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size=1000 examples\n",
      "Model has 2 layers, 3 heads, 15 words, 48 dimensions\n",
      "layer=0 \n",
      "  head=0\n",
      "    activation_name=v words=[1, 2, 3, 4, 6, 7, 8, 9]\n",
      "      word=1 n_c=9 err_c=0\n",
      "      word=2 n_c=10 err_c=0\n",
      "      word=3 n_c=10 err_c=0\n",
      "      word=4 n_c=10 err_c=0\n",
      "      word=6 n_c=9 err_c=0\n",
      "      word=7 n_c=10 err_c=0\n",
      "      word=8 n_c=10 err_c=0\n",
      "      word=9 n_c=10 err_c=0\n",
      "    activation_name=pattern-q words=[1, 5, 6, 7, 9, 10, 11, 12, 13]\n",
      "      word=1 n_c=9 err_c=0\n",
      "      word=5 n_c=100 err_c=0\n",
      "      word=6 n_c=45 err_c=0\n",
      "      word=7 n_c=95 err_c=19, n_m=113 err_m=0\n",
      "      word=9 n_c=79 err_c=27, n_m=105 err_m=0\n",
      "      word=10 n_c=24 err_c=35, n_m=58 err_m=0\n",
      "      word=11 n_c=19 err_c=27, n_m=45 err_m=0\n",
      "      word=12 n_c=53 err_c=112, n_m=164 err_m=0\n",
      "      word=13 n_c=64 err_c=0\n",
      "  head=1\n",
      "    activation_name=v words=[1, 2, 3, 4, 6, 7, 8, 9]\n",
      "      word=1 n_c=9 err_c=0\n",
      "      word=2 n_c=10 err_c=0\n",
      "      word=3 n_c=10 err_c=0\n",
      "      word=4 n_c=10 err_c=0\n",
      "      word=6 n_c=9 err_c=0\n",
      "      word=7 n_c=10 err_c=0\n",
      "      word=8 n_c=10 err_c=0\n",
      "      word=9 n_c=10 err_c=0\n",
      "    activation_name=pattern-q words=[1, 5, 6, 7, 9, 10, 11, 12, 13]\n",
      "      word=1 n_c=9 err_c=0\n",
      "      word=5 n_c=112 err_c=0\n",
      "      word=6 n_c=32 err_c=0\n",
      "      word=7 n_c=25 err_c=81, n_m=105 err_m=0\n",
      "      word=9 n_c=15 err_c=44, n_m=58 err_m=0\n",
      "      word=10 n_c=39 err_c=602, n_m=632 err_m=0\n",
      "      word=11 n_c=23 err_c=71, n_m=93 err_m=0\n",
      "      word=12 n_c=102 err_c=106, n_m=207 err_m=0\n",
      "      word=13 n_c=70 err_c=0\n",
      "  head=2\n",
      "    activation_name=v words=[1, 2, 3, 4, 6, 7, 8, 9]\n",
      "      word=1 n_c=9 err_c=0\n",
      "      word=2 n_c=10 err_c=0\n",
      "      word=3 n_c=10 err_c=0\n",
      "      word=4 n_c=10 err_c=0\n",
      "      word=6 n_c=9 err_c=0\n",
      "      word=7 n_c=10 err_c=0\n",
      "      word=8 n_c=10 err_c=0\n",
      "      word=9 n_c=10 err_c=0\n",
      "    activation_name=pattern-q words=[1, 5, 6, 7, 9, 10, 11, 12, 13]\n",
      "      word=1 n_c=9 err_c=0\n",
      "      word=5 n_c=92 err_c=0\n",
      "      word=6 n_c=42 err_c=36, n_m=77 err_m=0\n",
      "      word=7 n_c=26 err_c=34, n_m=59 err_m=0\n",
      "      word=9 n_c=73 err_c=0\n",
      "      word=10 n_c=31 err_c=629, n_m=657 err_m=0\n",
      "      word=11 n_c=15 err_c=95, n_m=109 err_m=0\n",
      "      word=12 n_c=51 err_c=0\n",
      "      word=13 n_c=35 err_c=0\n",
      "  activation_name=attn_out words=[1, 5, 6, 7, 9, 10, 11, 12, 13] \n",
      "      word=1 n_c=9 err_c=0\n",
      "      word=5 n_c=21 err_c=0\n",
      "      word=6 n_c=50 err_c=27, n_m=76 err_m=0\n",
      "      word=7 n_c=97 err_c=42, n_m=137 err_m=0\n",
      "      word=9 n_c=88 err_c=89, n_m=169 err_m=0\n",
      "      word=10 n_c=55 err_c=498, n_m=550 err_m=0\n",
      "      word=11 n_c=108 err_c=265, n_m=358 err_m=0\n",
      "      word=12 n_c=24 err_c=852, n_m=866 err_m=0\n",
      "      word=13 n_c=10 err_c=0\n",
      "layer=1 \n",
      "  head=0\n",
      "    activation_name=v words=[1, 5, 6, 7, 9, 10, 11, 12, 13]\n",
      "      word=1 n_c=9 err_c=0\n",
      "      word=5 n_c=21 err_c=0\n",
      "      word=6 n_c=49 err_c=0\n",
      "      word=7 n_c=98 err_c=0\n",
      "      word=9 n_c=87 err_c=19, n_m=105 err_m=0\n",
      "      word=10 n_c=12 err_c=36, n_m=47 err_m=0\n",
      "      word=11 n_c=51 err_c=52, n_m=101 err_m=0\n",
      "      word=12 n_c=11 err_c=0\n",
      "      word=13 n_c=93 err_c=0\n",
      "    activation_name=pattern-q words=[10, 11, 12, 13]\n",
      "      word=10 n_c=5 err_c=328, n_m=332 err_m=0\n",
      "      word=11 n_c=67 err_c=362, n_m=428 err_m=0\n",
      "      word=12 n_c=34 err_c=198, n_m=231 err_m=0\n",
      "      word=13 n_c=27 err_c=169, n_m=195 err_m=0\n",
      "  head=1\n",
      "    activation_name=v words=[1, 5, 6, 7, 9, 10, 11, 12, 13]\n",
      "      word=1 n_c=9 err_c=0\n",
      "      word=5 n_c=82 err_c=0\n",
      "      word=6 n_c=50 err_c=0\n",
      "      word=7 n_c=73 err_c=15, n_m=87 err_m=0\n",
      "      word=9 n_c=87 err_c=22, n_m=108 err_m=0\n",
      "      word=10 n_c=58 err_c=0\n",
      "      word=11 n_c=104 err_c=89, n_m=191 err_m=0\n",
      "      word=12 n_c=2 err_c=0\n",
      "      word=13 n_c=93 err_c=0\n",
      "    activation_name=pattern-q words=[10, 11, 12, 13]\n",
      "      word=10 n_c=103 err_c=95, n_m=197 err_m=0\n",
      "      word=11 n_c=13 err_c=124, n_m=136 err_m=0\n",
      "      word=12 n_c=65 err_c=217, n_m=281 err_m=0\n",
      "      word=13 n_c=65 err_c=102, n_m=166 err_m=0\n",
      "  head=2\n",
      "    activation_name=v words=[1, 5, 6, 7, 9, 10, 11, 12, 13]\n",
      "      word=1 n_c=9 err_c=0\n",
      "      word=5 n_c=15 err_c=0\n",
      "      word=6 n_c=48 err_c=27, n_m=74 err_m=0\n",
      "      word=7 n_c=98 err_c=24, n_m=121 err_m=0\n",
      "      word=9 n_c=87 err_c=16, n_m=102 err_m=0\n",
      "      word=10 n_c=34 err_c=340, n_m=373 err_m=0\n",
      "      word=11 n_c=102 err_c=69, n_m=169 err_m=0\n",
      "      word=12 n_c=5 err_c=0\n",
      "      word=13 n_c=94 err_c=0\n",
      "    activation_name=pattern-q words=[10, 11, 12, 13]\n",
      "      word=10 n_c=5 err_c=95, n_m=98 err_m=0\n",
      "      word=11 n_c=87 err_c=175, n_m=261 err_m=0\n",
      "      word=12 n_c=16 err_c=188, n_m=203 err_m=0\n",
      "      word=13 n_c=55 err_c=105, n_m=159 err_m=0\n",
      "  activation_name=attn_out words=[10, 11, 12, 13] \n",
      "      word=10 n_c=86 err_c=95, n_m=179 err_m=0\n",
      "      word=11 n_c=78 err_c=115, n_m=190 err_m=0\n",
      "      word=12 n_c=70 err_c=38, n_m=106 err_m=0\n",
      "      word=13 n_c=78 err_c=26, n_m=103 err_m=0\n"
     ]
    }
   ],
   "source": [
    "cluster_state = ClusterState(model,dataset.toks,min_samples=4,min_cluster_size=10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# activation_name=\"v\"\n",
    "# word=9 \n",
    "# layer=1 \n",
    "# head=2 \n",
    "\n",
    "\n",
    "# def test(word, data, activation_name, layer, head) :\n",
    "#     cluster_labels, silhouette_values, n_clusters, silhouette_avg, err = cluster_state.cluster_values( word, data=data, activation_name=activation_name, layer=layer, head=head )\n",
    "#     errs  =  err.sum().item()/err.shape[0]\n",
    "#     print( f\"word={word} layer={layer}  head={head} clusters={n_clusters} s_avg={silhouette_avg:.2f} err={errs:.0%}\")# unc={unc:.0%}\")\n",
    "#     total_err = err\n",
    "\n",
    "#     # max_count = 3\n",
    "#     # i = 0\n",
    "\n",
    "#     # while ( i < max_count and total_err.sum().item() > 2 ) :\n",
    "\n",
    "#     #     cluster_labels, silhouette_values, n_clusters, silhouette_avg, err = cluster_state.cluster_values( word, data=data, activation_name=activation_name, layer=layer, head=head, selection=total_err )\n",
    "#     #     errs  =  err.sum().item()/err.shape[0]\n",
    "#     #     print( f\"word={word} layer={layer}  head={head} clusters={n_clusters} s_avg={silhouette_avg:.2f} err={errs:.0%}  err_count={err.sum().item()}\")\n",
    "#     #     total_err[ total_err==True ] = err\n",
    "\n",
    "#     #     i=i+1\n",
    "\n",
    "# words = [1,5,6,7,9,10,11,12,13]\n",
    "\n",
    "# v = cluster_state.ctrl_cache[activation_name,layer]#[:,:,head,:]\n",
    "# v_out = einops.einsum(\n",
    "#     v, model.W_O[layer],\n",
    "#     \"batch posn_Q nheads d_head, nheads d_head d_model -> batch posn_Q nheads d_model\",\n",
    "# ) \n",
    "# # data = model.unembed(model.ln_final(v_out[:,:,head,:]))\n",
    "\n",
    "# for word in words :\n",
    "\n",
    "# #     # print(activation_name, v.shape)\n",
    "# #     # test(word, v[:,:,head,:], activation_name, layer, head)\n",
    "#     # print(activation_name+\"_out\", v_out.shape)\n",
    "#     test(word, v[:,:,head,:].detach(), activation_name, layer, head)\n",
    "#     test(word, v_out[:,:,head,:].detach(), activation_name, layer, head)\n",
    "\n",
    "# # for word in words :\n",
    "# #     print(activation_name+\"_data\", v_out.shape)\n",
    "# #     test(word, data.detach(), activation_name, layer, head)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def logistic_regression(X,y) :\n",
    "    # Load the Iris dataset\n",
    "    # iris = load_iris()\n",
    "    # X = iris.data  # Features: sepal length, sepal width, petal length, petal width\n",
    "    # print(X.shape)\n",
    "    # y = iris.target  # Target: species of Iris (setosa, versicolor, virginica)\n",
    "    # print(y.shape)\n",
    "    # print(y)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # It's a good practice to standardize the data (mean=0 and variance=1)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Create the logistic regression model\n",
    "    log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    log_reg.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = log_reg.predict(X_test)\n",
    "\n",
    "    # Output the accuracy of the model\n",
    "    print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n",
    "\n",
    "    # Output the confusion matrix\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "def linear_regression(X, y):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # It's a good practice to standardize the data (mean=0 and variance=1)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Create the linear regression model\n",
    "    lin_reg = LinearRegression()\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = lin_reg.predict(X_test)\n",
    "\n",
    "    # Output the Mean Squared Error of the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f'Mean Squared Error: {mse:.2f}')\n",
    "\n",
    "    # Output the R-squared score of the model\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f'R-squared: {r2:.2f}')\n",
    "\n",
    "    return lin_reg  # Optional: return the model for further use\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def pca_the_points(points, n_components=10, graph=True,labels=None):\n",
    "\n",
    "    n_components=min(n_components,points.size(0))\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    data = points.detach().cpu().numpy()\n",
    "    pca_result = pca.fit_transform(data)\n",
    "\n",
    "    # Get the two components\n",
    "    comp1 = pca.components_[0]\n",
    "    comp2 = pca.components_[1]\n",
    "\n",
    "    # Print the explained variance by each component\n",
    "    print(\"Explained variance by component:\", np.round(pca.explained_variance_ratio_,2)*100)\n",
    "\n",
    "    if graph:\n",
    "        # Plotting the explained variance\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.bar(range(n_components), pca.explained_variance_ratio_, align='center')\n",
    "        plt.xlabel(\"Principal Component\")\n",
    "        plt.ylabel(\"Explained Variance Ratio\")\n",
    "        plt.title(\"Explained Variance by Principal Component\")\n",
    "        plt.show()\n",
    "\n",
    "     # Create a bar chart\n",
    "        width = 0.4\n",
    "        indices = np.arange(48)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bar1 = plt.bar(indices, pca.components_[0], width, color='b', label='Principal Component 1')\n",
    "        bar2 = plt.bar(indices + width, pca.components_[1], width, color='r', label='Principal Component 2')\n",
    "        plt.xlabel(\"Dimensions\")\n",
    "        plt.ylabel(\"Weight\")\n",
    "        plt.title(\"Weights of Dimensions for First Two Principal Components\")\n",
    "        plt.xticks(indices + width / 2, indices)  # X-axis labels (centered)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        n=points.size(0) # n=pca_result.shape[0] number of points to plot\n",
    "\n",
    "        xi = 1\n",
    "        yi = 2\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.scatter(pca_result[:n, xi], pca_result[:n, yi], s=50, c='blue', edgecolors='k', marker='o', alpha=0.7)\n",
    "\n",
    "        if labels is  None :\n",
    "            for i in range(n) :\n",
    "                plt.annotate(i, (pca_result[i, xi], pca_result[i, yi]), fontsize=18, ha=\"right\")\n",
    "        else :\n",
    "            for i,label in enumerate(labels) :\n",
    "                plt.annotate( label , (pca_result[i, xi], pca_result[i, yi]), fontsize=18, ha=\"right\")\n",
    "\n",
    "        # Setting labels and title\n",
    "        plt.xlabel(f\"Principal Component {xi}\")\n",
    "        plt.ylabel(f\"Principal Component {yi}\")\n",
    "        # plt.xlim(-.7, .7)  # Set the x-axis limits \n",
    "        # plt.ylim(-.7, .7)  # Set the y-axis limits\n",
    "        plt.title(f\"Projection of Vectors on PC {xi} and PC {yi}\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    return comp1, comp2\n",
    "\n",
    "def show_gram_matrix(y, y_pred) :\n",
    "\n",
    "\n",
    "    y_l2_normalized      = F.normalize(y, p=2, dim=0)\n",
    "    y_pred_l2_normalized = F.normalize(y_pred, p=2, dim=0)\n",
    "\n",
    "    gram_matrix = t.mm( y_l2_normalized, y_pred_l2_normalized.t())\n",
    "\n",
    "    colorscale = [[0, 'white'], [.5, 'white'], [1.0, 'green']]\n",
    "\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=gram_matrix.detach().cpu().numpy(),\n",
    "        colorscale=colorscale,\n",
    "        zmin=-1,\n",
    "        zmax=1,\n",
    "        x=list(range(0, gram_matrix.size(-1))),  \n",
    "        y=list(range(0, gram_matrix.size(-1))),\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Gram Matrix Predicted vs Actual (correlation)',\n",
    "        xaxis_title='Actual Embedding position p-1',\n",
    "        yaxis_title='Predicted Embedding at position p-1',\n",
    "        yaxis_autorange='reversed', \n",
    "        autosize=False,\n",
    "        height=500,\n",
    "        width=500,\n",
    "    )\n",
    "    print(\"predicted embedding correlates highest with actual embedding\")\n",
    "    fig.show(\"png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()\n",
    "logits,cache  = model.run_with_cache(dataset.toks)\n",
    "preds =logits.softmax(dim=-1).argmax(dim=-1)\n",
    "model.reset_hooks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting digit -2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'head' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/alistair/code/ARENA_2.0/chapter1_transformers/exercises/monthly_algorithmic_problems/september23_sum/Alistair Fraser - Cluster 2.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alistair/code/ARENA_2.0/chapter1_transformers/exercises/monthly_algorithmic_problems/september23_sum/Alistair%20Fraser%20-%20Cluster%202.ipynb#X16sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# X1 = cache[\"result\",layer][:,seq1,head,:].detach().cpu().numpy()#[indices]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alistair/code/ARENA_2.0/chapter1_transformers/exercises/monthly_algorithmic_problems/september23_sum/Alistair%20Fraser%20-%20Cluster%202.ipynb#X16sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# X2 = cache[\"result\",layer][:,seq2,head,:].detach().cpu().numpy()#[indices]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alistair/code/ARENA_2.0/chapter1_transformers/exercises/monthly_algorithmic_problems/september23_sum/Alistair%20Fraser%20-%20Cluster%202.ipynb#X16sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m X  \u001b[39m=\u001b[39m X1 \u001b[39m#np.concatenate((X1,X2),axis=1)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alistair/code/ARENA_2.0/chapter1_transformers/exercises/monthly_algorithmic_problems/september23_sum/Alistair%20Fraser%20-%20Cluster%202.ipynb#X16sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhead=\u001b[39m\u001b[39m{\u001b[39;00mhead\u001b[39m}\u001b[39;00m\u001b[39m using words \u001b[39m\u001b[39m{\u001b[39;00mseq1\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00mseq2\u001b[39m}\u001b[39;00m\u001b[39m,  Logistic Regression predicts the \u001b[39m\u001b[39m{\u001b[39;00mdigit\u001b[39m}\u001b[39;00m\u001b[39m digit when there is no carrying\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alistair/code/ARENA_2.0/chapter1_transformers/exercises/monthly_algorithmic_problems/september23_sum/Alistair%20Fraser%20-%20Cluster%202.ipynb#X16sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# logistic_regression(X,y)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alistair/code/ARENA_2.0/chapter1_transformers/exercises/monthly_algorithmic_problems/september23_sum/Alistair%20Fraser%20-%20Cluster%202.ipynb#X16sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alistair/code/ARENA_2.0/chapter1_transformers/exercises/monthly_algorithmic_problems/september23_sum/Alistair%20Fraser%20-%20Cluster%202.ipynb#X16sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# err = cluster_state.substitute_centoids(layer, \"z\", head=0, lh_word=seq1, selection=t.tensor(indices[0]), save=False)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alistair/code/ARENA_2.0/chapter1_transformers/exercises/monthly_algorithmic_problems/september23_sum/Alistair%20Fraser%20-%20Cluster%202.ipynb#X16sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# print(f\"using words {seq1} and {seq2},  Logistic Regression predicts the {digit} digit when there is carrying\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alistair/code/ARENA_2.0/chapter1_transformers/exercises/monthly_algorithmic_problems/september23_sum/Alistair%20Fraser%20-%20Cluster%202.ipynb#X16sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# logistic_regression(X,y)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'head' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "layer = 0\n",
    "digit = -2\n",
    "output_digit = digit+1\n",
    "print(f\"Predicting digit {digit}\")\n",
    "\n",
    "remap = {}\n",
    "for i in range(Pairs.p.shape[0]) :\n",
    "    p = Pairs.p[i]\n",
    "    remap[i] = p[3]\n",
    "\n",
    "# for head in range(3) :\n",
    "\n",
    "seq1 = 9\n",
    "seq2 = -2\n",
    "y_orig = dataset.p.detach().cpu().numpy()\n",
    "yy  = np.array([ remap[y_orig[i]] for i in range(y_orig.shape[0]) ])\n",
    "# yy[ yy==1] = 0\n",
    "# yy[ yy==2] = 1\n",
    " \n",
    "indices = np.where(yy == 1)\n",
    "\n",
    "\n",
    "y  = yy\n",
    "\n",
    "# y1 = dataset.toks.detach().cpu().numpy()[:,4] + dataset.toks.detach().cpu().numpy()[:,9]\n",
    "# y = y1\n",
    "\n",
    "\n",
    "y = dataset.toks.detach().cpu().numpy()[indices][:,output_digit]\n",
    "X1 = cache[\"result\",layer][:,seq1,:,:].sum(1,keepdim=False).detach().cpu().numpy()#[indices]\n",
    "X2 = cache[\"result\",layer][:,seq2,:,:].sum(1,keepdim=False).detach().cpu().numpy()#[indices]\n",
    "\n",
    "\n",
    "# X1 = cache[\"result\",layer][:,seq1,head,:].detach().cpu().numpy()#[indices]\n",
    "# X2 = cache[\"result\",layer][:,seq2,head,:].detach().cpu().numpy()#[indices]\n",
    "\n",
    "X  = X1 #np.concatenate((X1,X2),axis=1)\n",
    "print(f\"head={head} using words {seq1} and {seq2},  Logistic Regression predicts the {digit} digit when there is no carrying\")\n",
    "# logistic_regression(X,y)\n",
    "\n",
    "# err = cluster_state.substitute_centoids(layer, \"z\", head=0, lh_word=seq1, selection=t.tensor(indices[0]), save=False)\n",
    "# print( f\"err={(err.sum().item()/err.shape[0]):.0%}\" ) \n",
    "    \n",
    "# err = cluster_state.substitute_centoids(layer, \"z\", head=1, lh_word=seq1, selection=t.tensor(indices[0]), save=False)\n",
    "# print( f\"err={(err.sum().item()/err.shape[0]):.0%}\" ) \n",
    "\n",
    "# err = cluster_state.substitute_centoids(layer, \"z\", head=2 ,lh_word=seq1, selection=t.tensor(indices[0]), save=False)\n",
    "# print( f\"err={(err.sum().item()/err.shape[0]):.0%}\" ) \n",
    "\n",
    "# indices = np.where(yy == 2)\n",
    "# y = dataset.toks.detach().cpu().numpy()[indices][:,output_digit]\n",
    "# X1 = cache[\"result\",layer][:,seq1,:,:].sum(1,keepdim=False).detach().cpu().numpy()[indices]\n",
    "# X2 = cache[\"result\",layer][:,seq2,:,:].sum(1,keepdim=False).detach().cpu().numpy()[indices]\n",
    "\n",
    "# # X1 = cache[\"result\",layer][:,seq1,head,:].detach().cpu().numpy()[indices]\n",
    "# # X2 = cache[\"result\",layer][:,seq2,head,:].detach().cpu().numpy()[indices]\n",
    "\n",
    "# X  = np.concatenate((X1,X2),axis=1)\n",
    "# print(f\"using words {seq1} and {seq2},  Logistic Regression predicts the {digit} digit when there is carrying\")\n",
    "# logistic_regression(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combine_cluster_labels(cluster_labels_list):\n",
    "    return t.stack(cluster_labels_list, dim=1).unique(dim=0, return_inverse=True)[1]\n",
    "\n",
    "def filter_words( pattern, threshold=.05 ) :\n",
    "    return t.where(  pattern.max(dim=0).values > threshold )[0].tolist()\n",
    "\n",
    "\n",
    "#head=1\n",
    "# activation_name=pattern-q words=[1, 5, 6, 7, 9, 10, 11, 12, 13]\n",
    "#   word=1 n_c=9 err_c=0, n_m=9 err_m=0.0\n",
    "#   word=5 n_c=44 err_c=0, n_m=631 err_m=0.0\n",
    "#   word=6 n_c=44 err_c=137, n_m=177 err_m=3\n",
    "#   word=7 n_c=43 err_c=59, n_m=102 err_m=3\n",
    "#   word=9 n_c=18 err_c=234, n_m=248 err_m=0\n",
    "\n",
    "#   word=10 n_c=7 err_c=166, n_m=174 err_m=21\n",
    "#   word=11 n_c=38 err_c=94, n_m=122 err_m=0\n",
    "\n",
    "#   word=12 n_c=39 err_c=209, n_m=232 err_m=0\n",
    "#   word=13 n_c=7 err_c=0, n_m=551 err_m=0.0\n",
    "\n",
    "head = 1\n",
    "seq1 = 10\n",
    "layer = 0\n",
    "def plot_it(data) :\n",
    "    data = data.detach().cpu().numpy() \n",
    "    plt.figure(figsize=(15, 5))  # Adjusting the figure size to avoid a skinny look\n",
    "    plt.imshow(data.T, aspect='auto')  # 'auto' aspect ratio makes the heatmap fit the figure\n",
    "    plt.colorbar()  # Adding a colorbar for reference\n",
    "    plt.xlabel('data points')\n",
    "    plt.ylabel('word')\n",
    "    plt.title(f'seq={seq1} Array')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# for seq1 in range(10,11) :\n",
    "for seq1 in range(1,14) :\n",
    "\n",
    "    pattern = cluster_state.ctrl_cache[\"pattern\",layer][:,head,seq1,:]\n",
    "\n",
    "    \n",
    "    cluster_labels, silhouette_values, n_clusters, silhouette_avg = cluster_state.find_clusters( pattern )\n",
    "    unc = (cluster_labels < 0).sum().item() / cluster_labels.shape[0]\n",
    "    print( f\"seq={seq1} n_clusters={n_clusters} silhouette_avg={silhouette_avg:.2f} unclassified={unc:.0%}\")\n",
    "    err = cluster_state.substitute_centoids(layer, \"pattern-q\", head=head, word=seq1, save=False)\n",
    "    print( f\"err={(err.sum().item()/err.shape[0]):.0%}\" ) \n",
    "\n",
    "    sorted_indices = t.argsort(cluster_labels)\n",
    "    pattern_sorted = pattern[sorted_indices]\n",
    "\n",
    "\n",
    "    words = filter_words( pattern )\n",
    "    print( f\"words={words}\" )\n",
    "    cluster_labels_list = [ cluster_state.result.get( (\"v\",layer,head,word),  t.zeros(pattern.shape[0]) ) for word in words ]\n",
    "    cluster_labels = combine_cluster_labels(cluster_labels_list)\n",
    "    print( f\" n_clusters={t.max(cluster_labels.unique()).item() + 1}\" )\n",
    "\n",
    "\n",
    "\n",
    "    plot_it(pattern)\n",
    "    plot_it(pattern_sorted)\n",
    "\n",
    "    filter = slice(None) #(cluster_labels < 0)\n",
    "\n",
    "    # err = cluster_state.substitute_centoids(layer, \"pattern-q\", head=head ,lh_word=seq1, save=False)\n",
    "    # print( f\"err={(err.sum().item()/err.shape[0]):.0%}\" ) \n",
    "\n",
    "    f = np.zeros((15,15))\n",
    "    X  = dataset.toks[filter,seq1-5].int()\n",
    "    Y  = dataset.toks[filter,seq1].int()\n",
    "    # Z  = pattern[filter,0]\n",
    "    # print(Z.shape)\n",
    "    # f[X, Y] = Z\n",
    "\n",
    "    np.add.at(f, (X, Y), 1)\n",
    "\n",
    "    plt.figure(figsize=(5,5))  # Adjusting the figure size to avoid a skinny look\n",
    "    plt.imshow(f, aspect='auto')  # 'auto' aspect ratio makes the heatmap fit the figure\n",
    "    plt.colorbar()  # Adding a colorbar for reference\n",
    "    plt.xlabel('X Axis ')\n",
    "    plt.ylabel('Y Axis ')\n",
    "    plt.title('Heatmap ')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tl_intro_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
