{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "import os;\n",
    "\n",
    "os.environ[\"ACCELERATE_ENABLE_RICH\"] = \"0\"\n",
    "\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = os.path.basename(globals()['__vsc_ipynb_file__']) \n",
    "\n",
    "# os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "\n",
    "import sys\n",
    "from functools import partial\n",
    "import json\n",
    "from typing import List, Tuple, Union, Optional, Callable, Dict\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import circuitsvis as cv\n",
    "import webbrowser\n",
    "from IPython.display import display\n",
    "from transformer_lens import utils, ActivationCache, HookedTransformer, HookedTransformerConfig\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens.components import LayerNorm\n",
    "from eindex import eindex\n",
    "\n",
    "\n",
    "from sklearn.cluster import DBSCAN, HDBSCAN, OPTICS\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# t.set_grad_enabled(False)\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = r\"chapter1_transformers\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = exercises_dir / \"monthly_algorithmic_problems\" / \"september23_sum\"\n",
    "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "from monthly_algorithmic_problems.september23_sum.model import create_model\n",
    "from monthly_algorithmic_problems.september23_sum.training import train, TrainArgs\n",
    "from monthly_algorithmic_problems.september23_sum.dataset import SumDataset,Pairs\n",
    "from plotly_utils import hist, bar, imshow\n",
    "\n",
    "# Running this on a macbook air and mps is flaky\n",
    "device = t.device(\"cpu\") #t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "-- Note: I rewrote the Dataset class so that it balances no carry, plain carry and cascading carry  classes evenly. this puts a larger emphasis difficult cases relative to the original way this dataset was written. With random additions, cascading carry is an edge case that is quite infrequent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SumDataset(size=1000, num_digits=4).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "-- Note: I removed weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = section_dir / \"sum_model_normal.pt\" # note this was trained on a mac in cpu mode without cuda and mps\n",
    "args = TrainArgs(\n",
    "    num_digits=4,\n",
    "    trainset_size=100_000,\n",
    "    valset_size=5_000,\n",
    "    epochs=100,\n",
    "    batch_size=512,\n",
    "    lr_start=2e-3,\n",
    "    lr_end=1e-4,\n",
    "    # weight_decay=0.001, # not weight decay, could add this back in \n",
    "    weight_decay=0.00,\n",
    "    seed=42,\n",
    "    d_model=48,\n",
    "    d_head=24,\n",
    "    n_layers=2,\n",
    "    n_heads=3,\n",
    "    d_mlp=None,\n",
    "    normalization_type=\"LN\",\n",
    "    use_wandb=True,\n",
    "    device=device,\n",
    ")\n",
    "model = create_model(\n",
    "    num_digits=4,\n",
    "    seed=0,\n",
    "    d_model=48,\n",
    "    d_head=24,\n",
    "    n_layers=2,\n",
    "    n_heads=3,\n",
    "    normalization_type=\"LN\",\n",
    "    d_mlp=None,\n",
    "    device=device\n",
    ")\n",
    "model.load_state_dict(t.load(filename))\n",
    "\n",
    "# model = train(args)\n",
    "# t.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "def _wh_hook(activation,hook,replacement_value,lh_word,head):\n",
    "\n",
    "    if (lh_word is not None and head is not None) : \n",
    "        activation[:, lh_word, head, :] = replacement_value\n",
    "    elif (lh_word is not None) :\n",
    "        activation[:, lh_word, :]  = replacement_value\n",
    "    else :\n",
    "        activation = replacement_value\n",
    "\n",
    "    return activation\n",
    "\n",
    "def wh_hook( replacement_value, lh_word = None, head = None) :\n",
    "    return functools.partial(_wh_hook,replacement_value=replacement_value, lh_word=lh_word, head=head)\n",
    "\n",
    "class ClusterState():\n",
    "    \"\"\"\n",
    "    A class to handle clustering and interpretation of transformer activations.\n",
    "\n",
    "    Attributes:\n",
    "    - model: HookedTransformer model.\n",
    "    - dataset: The dataset for the model.\n",
    "    - layers, heads, words, ndims: Model configuration parameters.\n",
    "    - pred_i: Indices of next-word predicted outputs.\n",
    "    - min_samples, min_cluster_size: Parameters for clustering.\n",
    "    - layer: The specific layer of the model to analyze.\n",
    "    - results: Stores results of clustering.\n",
    "\n",
    "    Methods:\n",
    "    - __init__: Constructor for the class.\n",
    "    - cluster_values: Clusters values and calculates error after substitution.\n",
    "    - generate_test_data: Generates a list of activations that can be clustered\n",
    "    - filter_words: Determines the words that impact predictions.\n",
    "    - calc_centroid: Calculates the centroid for each cluster\n",
    "    - find_clusters: Finds clusters in the data.\n",
    "    - substitute_centroid: Patches centroid values into transformer and tabluates errors.\n",
    "    - errors: Calculates prediction errors.\n",
    "    - interpret_cluster: Interprets and prints information about clusters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, dataset, min_samples=5, min_cluster_size=5, pred_i : List[int] = [10,11,12,13] , layer=1 ):\n",
    "        self.model   = model\n",
    "        self.dataset = dataset\n",
    "        self.layers  = model.cfg.n_layers\n",
    "        self.heads   = model.cfg.n_heads\n",
    "        self.words   = model.cfg.n_ctx\n",
    "        self.ndims   = model.cfg.d_model\n",
    "        self.pred_i  = pred_i\n",
    "        self.min_samples      = min_samples\n",
    "        self.min_cluster_size = min_cluster_size\n",
    "        self.layer = layer  \n",
    "\n",
    "        print(f\"Dataset size={dataset.shape[0]} examples\")\n",
    "        print(f\"Model has {self.layers} layers, {self.heads} heads, {self.words} words, {self.ndims} dimensions\")\n",
    "\n",
    "\n",
    "        model.reset_hooks()\n",
    "        self.ctrl_logits,self.ctrl_cache  = model.run_with_cache(dataset)\n",
    "        self.ctrl_preds = self.ctrl_logits.softmax(dim=-1).argmax(dim=-1)\n",
    "        model.reset_hooks()\n",
    "\n",
    "\n",
    "        self.filtered_words  = self.filter_words(layer)\n",
    "\n",
    "        print(f\"input words that do not impact predictions {[i for i in range(self.words) if i not in self.filtered_words]}\")\n",
    "\n",
    "        print(f\"input words that do impact predictions {self.filtered_words}\" )\n",
    "        \n",
    "        print(f\"found clusters layer={layer}\")\n",
    "\n",
    "        self.results = {}\n",
    "        for lh_word in self.filtered_words :\n",
    "            print(f\"word={lh_word}\")\n",
    "            all_test_data = self.generate_test_data(self.layer, lh_word=lh_word)\n",
    "            for data in all_test_data : \n",
    "                 cluster_labels, n_clusters, silhouette_avg, err = self.cluster_values(self.layer, lh_word, data=data[0], activation_name=data[1], head=data[2] )\n",
    "                 if (err.sum().item()==0) :\n",
    "                    print( f\"     head={data[2]} activation={data[1]} clusters={n_clusters} silhouette_avg={silhouette_avg:.2f} err={err.sum().item()}\")\n",
    "                    self.results[ ( lh_word, data[1], data[2]) ] = (cluster_labels, n_clusters, silhouette_avg, err)\n",
    "\n",
    "\n",
    "    def cluster_values(self, layer, lh_word, data=None, activation_name=None , head=None) :\n",
    "        cluster_labels, n_clusters, silhouette_avg = self.find_clusters(data)\n",
    "        rep_values = self.calc_centroid(data, cluster_labels)   \n",
    "        return cluster_labels, n_clusters, silhouette_avg, self.substitute_centoids(layer, activation_name, rep_values, lh_word=lh_word, head=head)\n",
    "\n",
    "    def generate_test_data(self, layer, lh_word=None) :\n",
    "        test_data  = []\n",
    "        for head in range(self.heads) :\n",
    "            test_data.append( (self.ctrl_cache[\"q\" , layer][:, lh_word, head, :], \"q\" , head)  )  \n",
    "            test_data.append( (self.ctrl_cache[\"k\" , layer][:, lh_word, head, :], \"k\" , head)  )    \n",
    "            test_data.append( (self.ctrl_cache[\"v\" , layer][:, lh_word, head, :], \"v\" , head)  )                \n",
    "        test_data.append( (self.ctrl_cache[\"normalized\" , layer, \"ln1\" ][:, lh_word, 0, :], \"normalized\" , 0)  )    \n",
    "        test_data.append( (self.ctrl_cache[\"resid_pre\"  , layer][:, lh_word, :], \"resid_pre\" , None)  )   \n",
    "        return test_data\n",
    "\n",
    "\n",
    "    def filter_words(self, layer, activation_name = \"resid_pre\") :\n",
    "        filtered_words    = []\n",
    "        for lh_word in range(self.words) :\n",
    "            r_value = self.ctrl_cache[activation_name, layer ][:,lh_word,:].mean(0,keepdim=True)\n",
    "            err = self.substitute_centoids( layer, activation_name , r_value, lh_word=lh_word ) \n",
    "            if (err.sum().item() > 0 ) :\n",
    "                filtered_words.append( lh_word )\n",
    "\n",
    "        return filtered_words\n",
    "\n",
    "    def calc_centroid(self, data, cluster_labels) :\n",
    "        centroids = t.vstack([data[cluster_labels == i].mean(0) for i in cluster_labels.unique()])\n",
    "        replacement_value = centroids[ cluster_labels ]\n",
    "        replacement_value[ cluster_labels < 0] = data[ cluster_labels < 0]\n",
    "        return replacement_value\n",
    "\n",
    "    def find_clusters(self,data):\n",
    "        data = data.numpy()\n",
    "        dbscan = HDBSCAN( min_samples=self.min_samples,min_cluster_size=self.min_cluster_size)\n",
    "        cluster_labels = dbscan.fit_predict(data)\n",
    "        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "        silhouette_avg = silhouette_score(data, cluster_labels) if n_clusters > 1 else 0\n",
    "        cluster_labels= t.from_numpy(cluster_labels).to(device)\n",
    "        return cluster_labels, n_clusters, silhouette_avg\n",
    "\n",
    "    def substitute_centoids(self, layer, activation_name, replacement_value, lh_word=None, head=None) :\n",
    "\n",
    "        self.model.reset_hooks()\n",
    "        if (activation_name==\"normalized\") :\n",
    "            self.model.add_hook( utils.get_act_name(activation_name, layer, \"ln1\"), wh_hook( replacement_value, lh_word, head)  )\n",
    "        else :\n",
    "            self.model.add_hook( utils.get_act_name(activation_name, layer), wh_hook( replacement_value, lh_word, head)  )\n",
    "\n",
    "        logits_ablate, _ = model.run_with_cache(self.dataset)\n",
    "        self.model.reset_hooks()\n",
    "        preds = logits_ablate.softmax(dim=-1).argmax(dim=-1)\n",
    "        err    = t.zeros( (self.dataset.shape[0]), dtype= t.bool)\n",
    "        for index, outcome_word in enumerate(self.pred_i) :\n",
    "            err =  err | self.errors( preds, outcome_word)  \n",
    "        return err\n",
    "\n",
    "    def errors(self, preds, outcome_word) :\n",
    "        return self.ctrl_preds[:,outcome_word] != preds[:,outcome_word]\n",
    "\n",
    "\n",
    "    # Note: See Pair class in dataset.py This prints out some information specific to the 4 digit addition problem\n",
    "    def interpret_cluster(self,word,activation_name,head=None) :\n",
    "\n",
    "        cluster_labels, n_clusters, silhouette_avg, err  = self.results[ ( word, activation_name, head) ] \n",
    "\n",
    "        print(f\"interpreting clusters at layer={self.layer} word={word} activation{activation_name} head={head}\")\n",
    "        print(f\"    clusters={n_clusters}, silhouette_avg={silhouette_avg:.2f} err={err.sum().item()}\")\n",
    "        for i in cluster_labels.unique() :\n",
    "\n",
    "            ds = dataset.toks[cluster_labels==i] \n",
    "            ps = dataset.p[cluster_labels==i]\n",
    "            print(\"\")\n",
    "            print(f\"        Cluster {i} - printing 5 of {ps.shape[0]} examples\")\n",
    "            for i in range(5) :\n",
    "                p = Pairs.p[ps[i].item()]\n",
    "                if p[1] == 2 or (p[1]==1 and p[2]==2) or (p[1]==1 and p[2]==1 and p[3]==2) :\n",
    "                    print( \"        [  carry ]\" , ds[i,:].detach().numpy() )\n",
    "                else :\n",
    "                    print( \"        [no carry]\" , ds[i,:].detach().numpy()  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size=1000 examples\n",
      "Model has 2 layers, 3 heads, 15 words, 48 dimensions\n",
      "input words that do not impact predictions [0, 2, 3, 4, 8, 14]\n",
      "input words that do impact predictions [1, 5, 6, 7, 9, 10, 11, 12, 13]\n",
      "found clusters layer=1\n",
      "word=1\n",
      "     head=0 activation=q clusters=9 silhouette_avg=1.00 err=0\n",
      "     head=0 activation=k clusters=9 silhouette_avg=1.00 err=0\n",
      "     head=0 activation=v clusters=9 silhouette_avg=1.00 err=0\n",
      "     head=1 activation=q clusters=9 silhouette_avg=1.00 err=0\n",
      "     head=1 activation=k clusters=9 silhouette_avg=1.00 err=0\n",
      "     head=1 activation=v clusters=9 silhouette_avg=1.00 err=0\n",
      "     head=2 activation=q clusters=9 silhouette_avg=1.00 err=0\n",
      "     head=2 activation=k clusters=9 silhouette_avg=1.00 err=0\n",
      "     head=2 activation=v clusters=9 silhouette_avg=1.00 err=0\n",
      "     head=0 activation=normalized clusters=9 silhouette_avg=1.00 err=0\n",
      "     head=None activation=resid_pre clusters=9 silhouette_avg=1.00 err=0\n",
      "word=5\n",
      "     head=0 activation=q clusters=16 silhouette_avg=0.62 err=0\n",
      "     head=1 activation=q clusters=19 silhouette_avg=0.55 err=0\n",
      "     head=1 activation=k clusters=16 silhouette_avg=0.50 err=0\n",
      "     head=1 activation=v clusters=20 silhouette_avg=0.62 err=0\n",
      "     head=2 activation=q clusters=17 silhouette_avg=0.64 err=0\n",
      "     head=2 activation=v clusters=11 silhouette_avg=0.85 err=0\n",
      "     head=0 activation=normalized clusters=18 silhouette_avg=0.60 err=0\n",
      "     head=None activation=resid_pre clusters=18 silhouette_avg=0.59 err=0\n",
      "word=6\n",
      "     head=0 activation=q clusters=34 silhouette_avg=0.75 err=0\n",
      "     head=1 activation=q clusters=34 silhouette_avg=0.76 err=0\n",
      "     head=2 activation=q clusters=35 silhouette_avg=0.74 err=0\n",
      "word=7\n",
      "     head=0 activation=q clusters=23 silhouette_avg=0.59 err=0\n",
      "     head=1 activation=q clusters=24 silhouette_avg=0.56 err=0\n",
      "     head=2 activation=q clusters=19 silhouette_avg=0.56 err=0\n",
      "word=9\n",
      "     head=0 activation=q clusters=19 silhouette_avg=0.40 err=0\n",
      "     head=1 activation=q clusters=20 silhouette_avg=0.45 err=0\n",
      "     head=2 activation=q clusters=17 silhouette_avg=0.40 err=0\n",
      "word=10\n",
      "     head=0 activation=k clusters=3 silhouette_avg=0.23 err=0\n",
      "     head=0 activation=v clusters=8 silhouette_avg=0.05 err=0\n",
      "     head=1 activation=q clusters=3 silhouette_avg=0.49 err=0\n",
      "     head=1 activation=k clusters=5 silhouette_avg=0.14 err=0\n",
      "     head=1 activation=v clusters=6 silhouette_avg=0.06 err=0\n",
      "word=11\n",
      "     head=1 activation=q clusters=6 silhouette_avg=0.45 err=0\n",
      "word=12\n",
      "     head=0 activation=k clusters=2 silhouette_avg=0.47 err=0\n",
      "     head=0 activation=v clusters=4 silhouette_avg=0.30 err=0\n",
      "     head=1 activation=k clusters=2 silhouette_avg=0.37 err=0\n",
      "     head=1 activation=v clusters=2 silhouette_avg=0.45 err=0\n",
      "     head=2 activation=k clusters=2 silhouette_avg=0.50 err=0\n",
      "     head=2 activation=v clusters=2 silhouette_avg=0.67 err=0\n",
      "word=13\n",
      "     head=0 activation=v clusters=19 silhouette_avg=0.32 err=0\n",
      "     head=1 activation=v clusters=15 silhouette_avg=0.32 err=0\n",
      "     head=2 activation=k clusters=24 silhouette_avg=0.43 err=0\n",
      "     head=2 activation=v clusters=19 silhouette_avg=0.36 err=0\n"
     ]
    }
   ],
   "source": [
    "cluster_state = ClusterState(model,dataset.toks,min_samples=5,min_cluster_size=20 )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interpreting clusters at layer=1 word=10 activationq head=1\n",
      "    clusters=3, silhouette_avg=0.49 err=0\n",
      "\n",
      "        Cluster 0 - printing 5 of 340 examples\n",
      "        [no carry] [12  3  1  9  0 10  4  5  7  7 11  7  7  6  7]\n",
      "        [no carry] [12  0  2  8  6 10  6  3  1  3 11  6  5  9  9]\n",
      "        [no carry] [12  6  0  1  0 10  0  3  8  5 11  6  3  9  5]\n",
      "        [no carry] [12  2  1  9  1 10  3  6  9  8 11  5  8  8  9]\n",
      "        [no carry] [12  3  1  9  1 10  4  5  9  9 11  7  7  9  0]\n",
      "\n",
      "        Cluster 1 - printing 5 of 320 examples\n",
      "        [no carry] [12  2  4  5  7 10  6  5  0  2 11  8  9  5  9]\n",
      "        [  carry ] [12  7  6  3  2 10  1  3  6  9 11  9  0  0  1]\n",
      "        [  carry ] [12  0  8  5  4 10  3  1  8  8 11  4  0  4  2]\n",
      "        [no carry] [12  0  2  1  6 10  7  7  4  5 11  7  9  6  1]\n",
      "        [  carry ] [12  3  1  4  8 10  1  8  9  9 11  5  0  4  7]\n",
      "\n",
      "        Cluster 2 - printing 5 of 340 examples\n",
      "        [  carry ] [12  5  6  2  5 10  3  7  5  5 11  9  3  8  0]\n",
      "        [  carry ] [12  0  7  4  2 10  2  3  5  8 11  3  1  0  0]\n",
      "        [  carry ] [12  7  8  5  2 10  1  3  4  8 11  9  2  0  0]\n",
      "        [  carry ] [12  4  5  2  9 10  2  6  4  8 11  7  1  7  7]\n",
      "        [  carry ] [12  3  9  7  1 10  2  6  9  6 11  6  6  6  7]\n"
     ]
    }
   ],
   "source": [
    "cluster_state.interpret_cluster( 10,\"q\", 1)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tl_intro_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
