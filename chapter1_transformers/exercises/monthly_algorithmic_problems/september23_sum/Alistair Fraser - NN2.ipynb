{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "import os;\n",
    "\n",
    "os.environ[\"ACCELERATE_ENABLE_RICH\"] = \"0\"\n",
    "\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = os.path.basename(globals()['__vsc_ipynb_file__']) \n",
    "\n",
    "# os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "\n",
    "import sys\n",
    "from functools import partial\n",
    "import json\n",
    "from typing import List, Tuple, Union, Optional, Callable, Dict\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import circuitsvis as cv\n",
    "import webbrowser\n",
    "from IPython.display import display\n",
    "from transformer_lens import utils, ActivationCache, HookedTransformer, HookedTransformerConfig\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens.components import LayerNorm\n",
    "from eindex import eindex\n",
    "\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# t.set_grad_enabled(False)\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = r\"chapter1_transformers\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = exercises_dir / \"monthly_algorithmic_problems\" / \"september23_sum\"\n",
    "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "from monthly_algorithmic_problems.september23_sum.model import create_model\n",
    "from monthly_algorithmic_problems.september23_sum.training import train, TrainArgs\n",
    "from monthly_algorithmic_problems.september23_sum.dataset import SumDataset,Pairs\n",
    "from plotly_utils import hist, bar, imshow\n",
    "\n",
    "# Running this on a macbook air and mps is flaky\n",
    "device = t.device(\"cpu\") #t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "-- Note: I rewrote the Dataset class so that it balances no carry, plain carry and cascading carry  classes evenly. this puts a larger emphasis difficult cases relative to the original way this dataset was written. With random additions, cascading carry is an edge case that is quite infrequent. \n",
    "\n",
    "-- I added a \"START\" token under the premise that it functions as an attention sink https://arxiv.org/pdf/2309.17453.pdf\n",
    "In truth, I really need to do more experiments to justify this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SumDataset(size=5000, num_digits=4).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "-- After doing this excercise, I also wanted to modify the attention mask because as you can see below, the loss function is only being calculated on the last num_digits. \n",
    "\n",
    "    def _shared_train_validation_step(self, toks: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        toks = toks.to(self.args.device)\n",
    "        logits = self.model(toks)[:, -(self.args.num_digits+1):-1]\n",
    "        target = toks[:, -self.args.num_digits:]\n",
    "        return logits, target\n",
    "\n",
    "-- FYI: I removed the weight_decay while I was playing around with adding a different form of regularization (minimizing the loss as a sum of covariance between activations of attention heads). I'm not including results from these tests at this point as there was a bit more work to do on it before I could make sense of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = section_dir / \"sum_model_normal.pt\" # note this was trained on a mac in cpu mode without cuda and mps\n",
    "args = TrainArgs(\n",
    "    num_digits=4,\n",
    "    trainset_size=100_000,\n",
    "    valset_size=5_000,\n",
    "    epochs=100,\n",
    "    batch_size=512,\n",
    "    lr_start=2e-3,\n",
    "    lr_end=1e-4,\n",
    "    # weight_decay=0.001, # not weight decay, could add this back in \n",
    "    weight_decay=0.00,\n",
    "    seed=42,\n",
    "    d_model=48,\n",
    "    d_head=24,\n",
    "    n_layers=2,\n",
    "    n_heads=3,\n",
    "    d_mlp=None,\n",
    "    normalization_type=\"LN\",\n",
    "    use_wandb=True,\n",
    "    device=device,\n",
    "    # corr_loss=CorrLoss(rank=2) # add this to add in a loss function of rank=2 for the correlation between activations on different attention heads\n",
    ")\n",
    "model = create_model(\n",
    "    num_digits=4,\n",
    "    seed=0,\n",
    "    d_model=48,\n",
    "    d_head=24,\n",
    "    n_layers=2,\n",
    "    n_heads=3,\n",
    "    normalization_type=\"LN\",\n",
    "    d_mlp=None,\n",
    "    device=device\n",
    ")\n",
    "model.load_state_dict(t.load(filename))\n",
    "\n",
    "# model = train(args)\n",
    "# t.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the dataset I'll use for the experiments below. \n",
    "- 1000 examples\n",
    "- Note: there is also data that tells you if a particular digit is carrying or if its a cascading carry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12,  3,  1,  9,  0, 10,  4,  5,  7,  7, 11,  7,  7,  6,  7])\n",
      "[0 0 2 0]\n",
      "torch.Size([1000, 15])\n"
     ]
    }
   ],
   "source": [
    "dataset  = SumDataset(size=1000, num_digits=4)\n",
    "print(dataset[0])\n",
    "reference = SumDataset(size=10 , num_digits=4)\n",
    "\n",
    "\n",
    "# Pairs shows you the case that the token sequence is testing \n",
    "# - a 0 means no carry\n",
    "# - a 1 means a '9' (which will trigger a cascading carry if there is a carry on the prior digit)\n",
    "# - a 2 means that the digit carrys over ot hte nest digit\n",
    "# [ 0 0 2 0 ] means no carry in the 1000s, 100s, 1s digit, while a carry in the 10s digit\n",
    "i = dataset.p[0].item()\n",
    "print(Pairs.p[i])\n",
    "print(dataset.toks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to do ablation runs\n",
    "\n",
    "-- The code below works, but the data structure I'm putting the results in is so unnecssarily complicated. It should be rewritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "def input_word_ablation_hook(resid_pre: Float[Tensor, \"batch seq d_model\"],hook: HookPoint,replacement_value,input_word: int) -> Float[Tensor, \"batch seq d_model\"]:\n",
    "    resid_pre[:, input_word, :] = replacement_value\n",
    "    return resid_pre\n",
    "\n",
    "def input_word_hook( replacement_value, input_word) :\n",
    "    return functools.partial(input_word_ablation_hook,replacement_value=replacement_value,input_word=input_word)\n",
    "\n",
    "def lh_word_ablation_hook(result: Float[Tensor, \"batch seq head_idx d_model\"],hook: HookPoint,replacement_value,head: int,lh_word: int) -> Float[Tensor, \"batch seq head_idx d_model\"]:\n",
    "    result[:, lh_word, head, :] = replacement_value\n",
    "    return result\n",
    "\n",
    "def lh_word_hook( replacement_value, head, lh_word) :\n",
    "    return functools.partial(lh_word_ablation_hook,replacement_value=replacement_value, head=head, lh_word=lh_word)\n",
    "\n",
    "\n",
    "def l_attn_ablation_hook(resid_post: Float[Tensor, \"batch seq d_model\"],hook: HookPoint,replacement_value,lh_word: int) -> Float[Tensor, \"batch seq d_model\"]:\n",
    "\n",
    "    if (lh_word >= 0 ) :\n",
    "        resid_post[:, lh_word, :] = replacement_value\n",
    "    else :\n",
    "        resid_post = replacement_value\n",
    "\n",
    "    return resid_post\n",
    "\n",
    "def l_attn_hook( replacement_value, lh_word) :\n",
    "    return functools.partial(l_attn_ablation_hook,replacement_value=replacement_value, lh_word=lh_word)\n",
    "\n",
    "# pass in dataset.toks\n",
    "class Ablation() :\n",
    "    def __init__(self, model : HookedTransformer, dataset : Dataset, pred_i : List[int] = [10,11,12,13] , reference : Dataset = None ) :\n",
    "        self.model   = model\n",
    "        self.dataset = dataset\n",
    "        self.layers  = model.cfg.n_layers\n",
    "        self.heads   = model.cfg.n_heads\n",
    "        self.words   = model.cfg.n_ctx\n",
    "        self.ndims   = model.cfg.d_model\n",
    "        self.pred_i  = pred_i\n",
    "        self.reference = reference \n",
    "\n",
    "        model.reset_hooks()\n",
    "        self.ctrl_logits,self.ctrl_cache  = model.run_with_cache(dataset)\n",
    "        self.ctrl_preds = self.ctrl_logits.softmax(dim=-1).argmax(dim=-1)\n",
    "        model.reset_hooks()\n",
    "\n",
    "\n",
    "        self.calc()\n",
    "\n",
    "\n",
    "    def calc(self) :    \n",
    "\n",
    "        if self.reference is not None :\n",
    "            model.reset_hooks()\n",
    "            self.ref_logits,self.ref_cache  = model.run_with_cache( reference )\n",
    "            self.ref_preds = self.ref_logits.softmax(dim=-1).argmax(dim=-1)\n",
    "            model.reset_hooks()\n",
    "            self.ref_length = reference.shape[0]\n",
    "        else :\n",
    "            self.ref_length = 1\n",
    "\n",
    "        self.tr_cache     = self.trace_inputs()\n",
    "\n",
    "        self.X = t.zeros( (self.ref_length, self.dataset.shape[0], len(self.pred_i), self.layers, self.heads, self.words, self.words), dtype= t.int)\n",
    "\n",
    "        self.ablate()\n",
    "\n",
    "\n",
    "\n",
    "    def trace_inputs(self) :\n",
    "        return { (ref, layer, input_word )  : self.calc_trace(ref, layer, input_word ) for ref in range(self.ref_length) for layer in range(self.layers)  for input_word in  range(self.words) }\n",
    " \n",
    "    def ablate(self):\n",
    "        [ self.calc_preds(ref, layer, head, lh_word, input_word ) for ref in range(self.ref_length) for layer in range(self.layers) for head in range(self.heads) for lh_word in self.layer_wordrange(layer) for input_word in  range(lh_word+1) ]\n",
    "\n",
    "    def layer_wordrange(self, layer) :\n",
    "        return range(self.words) if layer < self.layers else self.pred_i\n",
    "    \n",
    "    def calc_trace(self,ref, layer, input_word) :\n",
    "\n",
    "        if self.reference is None :\n",
    "            replacement_value = self.ctrl_cache[\"resid_pre\", layer][:,input_word,:].mean(0,keepdim=True)\n",
    "        else :\n",
    "            replacement_value = self.ref_cache[\"resid_pre\", layer][ ref ,input_word,:].unsqueeze(0)\n",
    "\n",
    "        self.model.reset_hooks()\n",
    "        self.model.add_hook( utils.get_act_name(\"resid_pre\", layer), input_word_hook( replacement_value ,input_word)  )\n",
    "        _ , t_cache = self.model.run_with_cache(self.dataset)\n",
    "        self.model.reset_hooks()\n",
    "        return t_cache\n",
    "\n",
    "    def calc_preds(self,ref, layer, head, lh_word, input_word) :\n",
    "\n",
    "\n",
    "        replacement_value = self.tr_cache[(ref,layer,input_word)][\"result\" , layer][:, lh_word, head, :]\n",
    "        # ctrl_value        = self.ctrl_cache[\"result\" , layer][:, lh_word, head, :]\n",
    "\n",
    "        self.model.reset_hooks()\n",
    "        self.model.add_hook( utils.get_act_name(\"result\", layer), lh_word_hook( replacement_value, head, lh_word)  )\n",
    "        logits_ablate, _ = model.run_with_cache(self.dataset)\n",
    "        self.model.reset_hooks()\n",
    "\n",
    "        #self.X = t.zeros( (dataset.toks.shape[0], len(self.pred_i), self.layers, self.heads, self.words, self.words), dtype= t.int)\n",
    "        preds = logits_ablate.softmax(dim=-1).argmax(dim=-1)\n",
    "\n",
    "        for index, outcome_word in enumerate(self.pred_i) :\n",
    "            self.X[ ref, self.errors( preds, outcome_word), index ,layer, head,lh_word,input_word] = 1\n",
    "\n",
    "        return None\n",
    "\n",
    "    def errors(self, preds, outcome_word) :\n",
    "        return self.ctrl_preds[:,outcome_word] != preds[:,outcome_word]\n",
    "        # return t.nonzero(self.ctrl_preds[:,outcome_word] != preds[:,outcome_word]).flatten()\n",
    "    \n",
    "\n",
    "    def visualize(self, title=None , outcome_words = None, labels=None, indices=None, ref = -1 , pr=False , layers = None) :\n",
    "        outcome_words = outcome_words or [0,1,2,3]\n",
    "        if (layers is None) :\n",
    "            layers = [ i in range(self.layers)]\n",
    "\n",
    "        if (indices is None) :\n",
    "            indices    = t.ones(self.X.shape[1], dtype=t.bool)\n",
    "        sz  = indices.sum().item()\n",
    "\n",
    "\n",
    "        def make_array( layer, head , outcome_word ) :\n",
    "            arr = np.zeros((self.words,self.words),dtype=float)\n",
    "            for lh_word in range(self.words) :\n",
    "                for input_word in range(lh_word+1) :\n",
    "                    if (ref < 0 ) :\n",
    "                        arr[input_word,lh_word] = self.X[ : , indices,outcome_word,layer,head,lh_word,input_word].sum() / (sz*self.X.shape[0])\n",
    "                    else :\n",
    "                        arr[input_word,lh_word] = self.X[ ref , indices,outcome_word,layer,head,lh_word,input_word].sum() / sz\n",
    "                    if (pr == True and arr[input_word,lh_word] > 0.005) :\n",
    "                        print(layer, head, input_word, lh_word ,arr[input_word,lh_word])\n",
    "            return arr\n",
    "\n",
    "        fig, axs = plt.subplots(len(layers),self.heads, figsize=(self.heads*3.5,len(layers)*3.5))\n",
    "        fig.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "        if title is not None :\n",
    "            fig.suptitle(title)\n",
    "\n",
    "        cmaps = [ 'Reds','Blues','Purples','Greens']\n",
    "\n",
    "        for layer in layers:\n",
    "            for head in range(self.heads):\n",
    "                arr = []\n",
    "                for digit in outcome_words :\n",
    "                    arr.append(  make_array( layer, head, digit )  )\n",
    "\n",
    "                ax = axs[layer, head] if len(layers) > 1 else axs[head]\n",
    "\n",
    "                ax.set_title(f'Layer {layer}, Head {head}')\n",
    "                ax.set_yticks(np.arange(-.5, self.words-.5,1), minor=True)\n",
    "                ax.set_yticks(np.arange(self.words), minor=False)\n",
    "\n",
    "                if (head==0) :\n",
    "                    ax.set_ylabel('Inputted Words')\n",
    "                    if (labels is not None) :\n",
    "                        ax.set_yticklabels(labels)\n",
    "                else :\n",
    "                    ax.set_yticklabels([])\n",
    "\n",
    "                if (layer == len(layers)-1) :\n",
    "                    ax.set_xlabel('\\\"Result\\\" words')\n",
    "                    \n",
    "\n",
    "                ax.set_xticks(np.arange(-.5, self.words-.5,1), minor=True)\n",
    "                ax.set_xticks(np.arange(0,self.words,1), minor=False)\n",
    "                if (labels is not None) :\n",
    "                    ax.set_xticklabels(labels)\n",
    "                \n",
    "                ax.invert_yaxis()\n",
    "\n",
    "                # Draw horizontal grid lines\n",
    "                ax.hlines(np.arange(-.5, self.words-.5,1), *ax.get_xlim(), color='grey', linewidth=1)\n",
    "                # Draw vertical grid lines\n",
    "                ax.vlines(np.arange(-.5, self.words-.5,1), *ax.get_ylim(), color='grey', linewidth=1)\n",
    "\n",
    "                def offset(d_in,i,j) :\n",
    "                    off = False\n",
    "                    for dd in range(d_in) :\n",
    "                        if arr[dd][i,j] > 0.005 :\n",
    "                            off = True\n",
    "                            break\n",
    "                    return off\n",
    "                \n",
    "                cax = ax.imshow(np.zeros((self.words,self.words),dtype=float), cmap='Greys',aspect='equal',vmin=0,vmax=1)\n",
    "                for i in range(arr[0].shape[0]):\n",
    "                    for j in range(arr[0].shape[1]):\n",
    "                        for d in range(len(outcome_words)) : \n",
    "                            cmap = plt.get_cmap(cmaps[d]) \n",
    "                            if (arr[d][i,j]>.005) :\n",
    "                                darkcolor = cmap(1.0)\n",
    "                                color = cmap(arr[d][i, j])\n",
    "                                if offset(d,i,j) :\n",
    "                                    rect = Rectangle((j - 0.5 + .35, i - 0.5 + .35), .9, .9, linewidth=2, edgecolor=darkcolor, facecolor=color)\n",
    "                                else :\n",
    "                                    rect = Rectangle((j - 0.5      , i - 0.5      ), 1.0, 1.0, linewidth=2, edgecolor=darkcolor, facecolor=color)     \n",
    "                                ax.add_patch(rect)\n",
    "\n",
    "                ax.tick_params(axis='x', which='major', length=0)\n",
    "                ax.tick_params(axis='y', which='major', length=0)\n",
    "                ax.tick_params(axis='x', which='minor', length=0)\n",
    "                ax.tick_params(axis='y', which='minor', length=0)\n",
    "            \n",
    "        plt.subplots_adjust(hspace=0.5) \n",
    "        plt.show()\n",
    "\n",
    "from sklearn.cluster import DBSCAN, HDBSCAN, OPTICS\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "class ClusterState(Ablation):\n",
    "\n",
    "    # def __init__(self, d=None, *args, **kwargs):\n",
    "    def __init__(self, *args, min_samples=5, min_cluster_size=5, lh_word=10,  **kwargs):\n",
    "        self.lh_word = lh_word\n",
    "        self.min_samples = min_samples\n",
    "        self.min_cluster_size = min_cluster_size\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def calc(self) :\n",
    "\n",
    "        self.layer = 0\n",
    "\n",
    "        self.replacement_value, self.filtered_words  = self.filter_words()\n",
    "        self.ctrl_data = self.ctrl_cache[\"resid_post\" , self.layer][:, self.filtered_words, :].reshape( self.dataset.shape[0], self.filtered_words.sum()*self.ndims )\n",
    "        \n",
    "        err = None\n",
    "        for i in range(5) :\n",
    "            if (err is None or err.sum() > self.min_cluster_size ) :\n",
    "                err = self.replace_values(err=err)\n",
    "                print( f\"round {i} err={ (err.sum().item() / err.shape[0]):.0%} clusters={self.n_clusters} unlassified={self.cluster_labels[self.cluster_labels < 0].shape[0]/self.cluster_labels.shape[0]:.0%}\" )\n",
    "            else:\n",
    "                break\n",
    "        if (err.sum() > 0 ) :\n",
    "            self.cluster_labels[err]    = -1 \n",
    "            self.copy_to_replacement_values(self.ctrl_data[err], err)\n",
    "            err = self.substitute_centroids(self.layer, -1 , self.replacement_value)\n",
    "        \n",
    "        print( f\"round f err={ (err.sum().item() / err.shape[0]):.0%} clusters={self.n_clusters} unlassified={self.cluster_labels[self.cluster_labels < 0].shape[0]/self.cluster_labels.shape[0]:.0%}\" )\n",
    "        print(\"\")\n",
    "\n",
    "    def copy_to_replacement_values(self, rep_values, err )  :\n",
    "        self.rep_values[err] = rep_values\n",
    "        true_words = self.filtered_words.nonzero().squeeze()\n",
    "        r_value  = self.rep_values[err].reshape( -1 , self.filtered_words.sum() , self.ndims )\n",
    "        for w,word in enumerate(true_words.tolist()) :\n",
    "            self.replacement_value[ err , word , : ] = r_value[ :, w, :]\n",
    "\n",
    "\n",
    "    def filter_words(self) :\n",
    "\n",
    "        replacement_value = t.clone(self.ctrl_cache[\"resid_post\", self.layer ])\n",
    "        filtered_words    = t.ones(self.words,dtype=t.bool)\n",
    "\n",
    "        for word in range(self.words) :\n",
    "            r_value = self.ctrl_cache[\"resid_post\", self.layer ][:,word,:].mean(0,keepdim=True)\n",
    "            err = self.substitute_centroids(self.layer, word , r_value) \n",
    "            if (err.sum().item() == 0 ) :\n",
    "                replacement_value[:,word,:] = r_value\n",
    "                filtered_words[word]        = False\n",
    "\n",
    "        return replacement_value, filtered_words\n",
    "\n",
    "    def replace_values(self,err=None) :\n",
    "\n",
    "        data = self.ctrl_data\n",
    "\n",
    "        if err is None : \n",
    "            self.cluster_labels, self.n_clusters, silhouette_avg = self.find_clusters(data)\n",
    "            self.rep_values = self.calc_replacement_values(data, self.cluster_labels)\n",
    "            self.replacement_value[ :, self.filtered_words,: ] = self.rep_values.reshape( self.dataset.shape[0] , self.filtered_words.sum() , self.ndims )\n",
    "        else :\n",
    "            cluster_labels, n_clusters, silhouette_avg = self.find_clusters( data[err] )\n",
    "            rep_values = self.calc_replacement_values( data[err], cluster_labels)\n",
    "            cluster_labels[cluster_labels >= 0] = cluster_labels[cluster_labels >= 0] + self.n_clusters\n",
    "            self.cluster_labels[err] = cluster_labels \n",
    "            self.n_clusters = self.cluster_labels.unique().max().item() + 1\n",
    "            self.copy_to_replacement_values(rep_values, err)\n",
    "        \n",
    "        return self.substitute_centroids(self.layer, -1 , self.replacement_value)\n",
    "\n",
    "\n",
    "    def calc_replacement_values(self, data, cluster_labels) :\n",
    "        centroids = t.vstack([data[cluster_labels == i].mean(0) for i in cluster_labels.unique()])\n",
    "        replacement_value = centroids[ cluster_labels ]\n",
    "        replacement_value[ cluster_labels < 0] = data[ cluster_labels < 0]\n",
    "        return replacement_value\n",
    "\n",
    "\n",
    "    def find_clusters(self,data):\n",
    "        data = data.numpy()\n",
    "        dbscan = HDBSCAN( min_samples=self.min_samples,min_cluster_size=self.min_cluster_size)\n",
    "        cluster_labels = dbscan.fit_predict(data)\n",
    "        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "        silhouette_avg = silhouette_score(data, cluster_labels) if n_clusters > 1 else 0\n",
    "        cluster_labels= t.from_numpy(cluster_labels).to(device)\n",
    "        return cluster_labels, n_clusters, silhouette_avg\n",
    "\n",
    "\n",
    "\n",
    "    def substitute_centroids(self,layer, lh_word , replacement_value) :\n",
    "\n",
    "        self.model.reset_hooks()\n",
    "        self.model.add_hook( utils.get_act_name(\"resid_post\", layer), l_attn_hook( replacement_value, lh_word)  )\n",
    "        logits_ablate, _ = model.run_with_cache(self.dataset)\n",
    "        self.model.reset_hooks()\n",
    "        preds = logits_ablate.softmax(dim=-1).argmax(dim=-1)\n",
    "        err    = t.zeros( (self.dataset.shape[0]), dtype= t.bool)\n",
    "        for index, outcome_word in enumerate(self.pred_i) :\n",
    "            err =  err | self.errors( preds, outcome_word)  \n",
    "        return err\n",
    "\n",
    "\n",
    "    def visualize_all(self) :\n",
    "        labels = [\"ST\",\"0\",\"1\",\"2\",\"3\",\"+\",\"4\",\"5\",\"6\",\"7\",\"=\",\"4\",\"6\",\"9\",\"0\"]\n",
    "\n",
    "        for i in range(self.n_clusters) :\n",
    "            self.visualize(title=f\"Cluster {i}\",outcome_words=[0],labels=labels,indices=self.cluster_labels==i,ref=-1,pr=False,layers=[0])\n",
    "            ds = dataset.toks[self.cluster_labels==i] \n",
    "            ps = dataset.p[self.cluster_labels==i]\n",
    "            print(ds.shape,ps.shape)\n",
    "            for i in range(ds.shape[0]) :\n",
    "                p = Pairs.p[ps[i].item()]\n",
    "                if p[1] == 2 or (p[1]==1 and p[2]==2) or (p[1]==1 and p[2]==1 and p[3]==2) :\n",
    "                    print( \"   carry\" , ds[i,1] + ds[i,6],ds[i,:] ,  p)\n",
    "                else :\n",
    "                    print( \"no carry\" , ds[i,1] + ds[i,6], ds[i,:] ,  p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 15, 48]) torch.Size([15]) 3\n",
      "self.ctrl_data torch.Size([1000, 144])\n",
      "round 0 err=58% clusters=265 unlassified=12%\n",
      "round 1 err=43% clusters=425 unlassified=13%\n",
      "round 2 err=36% clusters=541 unlassified=13%\n",
      "round 3 err=31% clusters=634 unlassified=13%\n",
      "round 4 err=27% clusters=715 unlassified=13%\n",
      "round f err=0% clusters=715 unlassified=40%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_state = ClusterState(model,dataset.toks,min_samples=2,min_cluster_size=2,pred_i=[10])\n",
    "# cluster_state.visualize_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  4  7]\n",
      " [20 23 26]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Create a 3D array\n",
    "array_3d = np.arange(27).reshape((3, 3, 3))\n",
    "\n",
    "# Create boolean index arrays for two of the dimensions\n",
    "bool_idx1 = np.array([True, False, True])\n",
    "bool_idx2 = np.array([False, True, True])\n",
    "\n",
    "# Index into the 3D array\n",
    "# This will select the 1st and 3rd slice of the 1st dimension\n",
    "# and the 2nd and 3rd slice of the 2nd dimension, all elements in the 3rd dimension\n",
    "result = array_3d[bool_idx1, :, bool_idx2]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_state = ClusterState(model,dataset.toks,min_samples=3,min_cluster_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_words = [ 10, 12, 13, 11 ]\n",
    "labels = [\"ST\",\"0\",\"1\",\"2\",\"3\",\"+\",\"4\",\"5\",\"6\",\"7\",\"=\",\"4\",\"6\",\"9\",\"0\"]\n",
    "\n",
    "ablation = Ablation(model,dataset.toks)#,reference=reference.toks)\n",
    "ablation.visualize(title=\"Ablation of the input words for the  model\",outcome_words=[0,1,2,3],labels=labels,ref=-1,layers=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def logistic_regression(X,y) :\n",
    "    # Load the Iris dataset\n",
    "    # iris = load_iris()\n",
    "    # X = iris.data  # Features: sepal length, sepal width, petal length, petal width\n",
    "    # print(X.shape)\n",
    "    # y = iris.target  # Target: species of Iris (setosa, versicolor, virginica)\n",
    "    # print(y.shape)\n",
    "    # print(y)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # It's a good practice to standardize the data (mean=0 and variance=1)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Create the logistic regression model\n",
    "    log_reg = LogisticRegression(random_state=42, max_iter=200)\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    log_reg.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = log_reg.predict(X_test)\n",
    "\n",
    "    # Output the accuracy of the model\n",
    "    print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n",
    "\n",
    "    # Output the confusion matrix\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "def linear_regression(X, y):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # It's a good practice to standardize the data (mean=0 and variance=1)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Create the linear regression model\n",
    "    lin_reg = LinearRegression()\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = lin_reg.predict(X_test)\n",
    "\n",
    "    # Output the Mean Squared Error of the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f'Mean Squared Error: {mse:.2f}')\n",
    "\n",
    "    # Output the R-squared score of the model\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f'R-squared: {r2:.2f}')\n",
    "\n",
    "    return lin_reg  # Optional: return the model for further use\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def pca_the_points(points, n_components=10, graph=True,labels=None):\n",
    "\n",
    "    n_components=min(n_components,points.size(0))\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    data = points.detach().cpu().numpy()\n",
    "    pca_result = pca.fit_transform(data)\n",
    "\n",
    "    # Get the two components\n",
    "    comp1 = pca.components_[0]\n",
    "    comp2 = pca.components_[1]\n",
    "\n",
    "    # Print the explained variance by each component\n",
    "    print(\"Explained variance by component:\", np.round(pca.explained_variance_ratio_,2)*100)\n",
    "\n",
    "    if graph:\n",
    "        # Plotting the explained variance\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.bar(range(n_components), pca.explained_variance_ratio_, align='center')\n",
    "        plt.xlabel(\"Principal Component\")\n",
    "        plt.ylabel(\"Explained Variance Ratio\")\n",
    "        plt.title(\"Explained Variance by Principal Component\")\n",
    "        plt.show()\n",
    "\n",
    "     # Create a bar chart\n",
    "        width = 0.4\n",
    "        indices = np.arange(48)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bar1 = plt.bar(indices, pca.components_[0], width, color='b', label='Principal Component 1')\n",
    "        bar2 = plt.bar(indices + width, pca.components_[1], width, color='r', label='Principal Component 2')\n",
    "        plt.xlabel(\"Dimensions\")\n",
    "        plt.ylabel(\"Weight\")\n",
    "        plt.title(\"Weights of Dimensions for First Two Principal Components\")\n",
    "        plt.xticks(indices + width / 2, indices)  # X-axis labels (centered)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        n=points.size(0) # n=pca_result.shape[0] number of points to plot\n",
    "\n",
    "        xi = 1\n",
    "        yi = 2\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.scatter(pca_result[:n, xi], pca_result[:n, yi], s=50, c='blue', edgecolors='k', marker='o', alpha=0.7)\n",
    "\n",
    "        if labels is  None :\n",
    "            for i in range(n) :\n",
    "                plt.annotate(i, (pca_result[i, xi], pca_result[i, yi]), fontsize=18, ha=\"right\")\n",
    "        else :\n",
    "            for i,label in enumerate(labels) :\n",
    "                plt.annotate( label , (pca_result[i, xi], pca_result[i, yi]), fontsize=18, ha=\"right\")\n",
    "\n",
    "        # Setting labels and title\n",
    "        plt.xlabel(f\"Principal Component {xi}\")\n",
    "        plt.ylabel(f\"Principal Component {yi}\")\n",
    "        # plt.xlim(-.7, .7)  # Set the x-axis limits \n",
    "        # plt.ylim(-.7, .7)  # Set the y-axis limits\n",
    "        plt.title(f\"Projection of Vectors on PC {xi} and PC {yi}\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    return comp1, comp2\n",
    "\n",
    "def show_gram_matrix(y, y_pred) :\n",
    "\n",
    "\n",
    "    y_l2_normalized      = F.normalize(y, p=2, dim=0)\n",
    "    y_pred_l2_normalized = F.normalize(y_pred, p=2, dim=0)\n",
    "\n",
    "    gram_matrix = t.mm( y_l2_normalized, y_pred_l2_normalized.t())\n",
    "\n",
    "    colorscale = [[0, 'white'], [.5, 'white'], [1.0, 'green']]\n",
    "\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=gram_matrix.detach().cpu().numpy(),\n",
    "        colorscale=colorscale,\n",
    "        zmin=-1,\n",
    "        zmax=1,\n",
    "        x=list(range(0, gram_matrix.size(-1))),  \n",
    "        y=list(range(0, gram_matrix.size(-1))),\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Gram Matrix Predicted vs Actual (correlation)',\n",
    "        xaxis_title='Actual Embedding position p-1',\n",
    "        yaxis_title='Predicted Embedding at position p-1',\n",
    "        yaxis_autorange='reversed', \n",
    "        autosize=False,\n",
    "        height=500,\n",
    "        width=500,\n",
    "    )\n",
    "    print(\"predicted embedding correlates highest with actual embedding\")\n",
    "    fig.show(\"png\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tl_intro_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
